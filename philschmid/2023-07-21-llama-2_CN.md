# LLaMA 2 - 你所需要的一切资源

> ## 摘录
> 关于LLaMA 2的全部资源，如何去测试，训练，并部署它。

---
LLaMA 2是一个由Meta开发的大型语言模型，是LLaMA 1的继任者。LLaMA 2可通过AWS、Hugging Face等提供商获取，并免费用于研究和商业用途。LLaMA 2预训练模型在2万亿个标记上进行训练，相比LLaMA 1的上下文长度增加了一倍。它的微调模型则在超过100万个人工标注数据下完成。

这篇博客包含了所有的相关资源，以帮助您快速入门。它包括以下链接：

-   [LLaMA 2是什么？](https://www.philschmid.de/llama-2#what-is-llama-2)
-   [游乐场，你可以在这里测试模型](https://www.philschmid.de/llama-2#llama-playgrounds-test-it)
-   [模型背后的研究工作](https://www.philschmid.de/llama-2#research-behind-llama-2)
-   [模型性能有多好，基准测试](https://www.philschmid.de/llama-2#how-good-is-llama-2-benchmarks)
-   [如何正确地去提示聊天模型](https://www.philschmid.de/llama-2#how-to-prompt-llama-2-chat)
-   [如何使用PEFT训练模型](https://www.philschmid.de/llama-2#how-to-train-llama-2)
-   [如何部署模型进行推理](https://www.philschmid.de/llama-2#how-to-deploy-llama-2)
-   [和其他资源](https://www.philschmid.de/llama-2#other-sources)


来自Meta官方的公告可以在这里找到：[https://ai.meta.com/llama/](https://ai.meta.com/llama/)

## LLaMA 2是什么？

Meta发布的LLaMA 2，是新的sota开源大型语言模型（LLM）。LLaMA 2代表着LLaMA的下一代版本，并且具有商业许可证。LLaMA 2有3种不同的大小——7B、13B和70B个可训练参数。与原版LLaMA相比，新的改进包括：

-   在2万亿个标记的文本数据上进行训练
-   允许商业使用
-   默认使用4096个前后文本视野（[可以被扩展](https://twitter.com/joao_gante/status/1681593605541236736?s=20)）
-   70B模型采用了分组查询注意力（GQA）
-   可由此获取[Hugging Face Hub](https://huggingface.co/models?other=llama-2)

## 在LLaMA 游乐场，测试它

有几个不同的游乐场供与LLaMA 2来测试聊天：
There are a few different playgrounds available to test out interacting with LLaMA 2 Chat:

-   [HuggingChat](https://huggingface.co/chat) 允许你通过Hugging Face的对话界面与LLaMA 2 70B模型聊天。这提供了一个简洁的方法来了解聊天机器人的工作原理。
-   [Hugging Face Spaces](https://huggingface.co/spaces) 有三种大小的LLaMA 2模型[7B](https://huggingface.co/spaces/huggingface-projects/llama-2-7b-chat)、[13B](https://huggingface.co/spaces/huggingface-projects/llama-2-13b-chat)和[70B](https://huggingface.co/spaces/ysharma/Explore_llamav2_with_TGI)可供测试。交互式演示可以让您比较不同的大小模型的区别。
-   [Perplexity](https://llama.perplexity.ai/) 他们的对话AI演示提供7B和13B的LLaMA 2模型。你可以与模型聊天并且反馈模型响应的不足。

## Research Behind LLaMA 2
## LLaMA 2背后的研究工作

LLaMA 2 is a base LLM model and pretrained on publicly available data found online. Additionally Meta released a CHAT version. The first version of the CHAT model was SFT (Supervised fine-tuned) model. After that, LLaMA-2-chat was iteratively improved through Reinforcement Learning from Human Feedback (RLHF). The RLHF process involved techniques like rejection sampling and proximal policy optimization (PPO) to further refine the chatbot. Meta only released the latest RLHF (v5) versions of the model. If you curious how the process was behind checkout:

-   [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)
-   [Llama 2: an incredible open LLM](https://www.interconnects.ai/p/llama-2-from-meta)
-   [Llama 2: Full Breakdown](https://www.youtube.com/watch?v=zJBpRn2zTco&ab_channel=AIExplained)

## How good is LLaMA 2, benchmarks?

Meta claims that _“Llama 2 outperforms other open source language models on many external benchmarks, including reasoning, coding, proficiency, and knowledge tests.”._ You can find more insights over the performance at:

-   [Hugging Face Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
-   [Meta Announcement](https://ai.meta.com/llama/)

## How to Prompt LLaMA 2 Chat

LLaMA 2 Chat is an open conversational model. Interacting with LLaMA 2 Chat effectively requires providing the right prompts and questions to produce coherent and useful responses. Meta didn’t choose the simplest prompt. Below is the prompt template for single-turn and multi-turn conversations. This template follows the model's training procedure, as described in [the LLaMA 2 paper](https://huggingface.co/papers/2307.09288). You can also take a look at [LLaMA 2 Prompt Template](https://gpus.llm-utils.org/llama-2-prompt-template/).

Single-turn

```
<s>[INST] <<SYS>>
{{ system_prompt }}
<</SYS>>

{{ user_message }} [/INST]
```

Multi-turn

```
<s>[INST] <<SYS>>
{{ system_prompt }}
<</SYS>>

{{ user_msg_1 }} [/INST] {{ model_answer_1 }} </s><s>[INST] {{ user_msg_2 }} [/INST] {{ model_answer_2 }} </s><s>[INST] {{ user_msg_3 }} [/INST]
```

## How to train LLaMA 2

LLaMA 2 is openly available making it easy to fine-tune using techniques, .e.g. PEFT. There are great resources available for training your own versions of LLaMA 2:

-   [Extended Guide: Instruction-tune Llama 2](https://www.philschmid.de/instruction-tune-llama-2)
-   [Fine-tune LLaMA 2 (7-70B) on Amazon SageMaker](https://www.philschmid.de/sagemaker-llama2-qlora)
-   [Fine-tuning with PEFT](https://huggingface.co/blog/llama2#fine-tuning-with-peft)
-   [Meta Examples and recipes for Llama model](https://github.com/facebookresearch/llama-recipes/tree/main)
-   [The EASIEST way to finetune LLAMA-v2 on local machine!](https://www.youtube.com/watch?v=3fsn19OI_C8&ab_channel=AbhishekThakur)

## How to Deploy LLaMA 2

LLaMA 2 can be deployed in local environment ([llama.cpp](https://github.com/ggerganov/llama.cpp)), using managed services like [Hugging Face Inference Endpoints](https://ui.endpoints.huggingface.co/) or through or cloud platforms like AWS, Google Cloud, and Microsoft Azure.

-   [Deploy LLaMa 2 Using text-generation-inference and Inference Endpoints](https://huggingface.co/blog/llama2#using-text-generation-inference-and-inference-endpoints)
-   Deploy LLaMA 2 70B using Amazon SageMaker (coming soon)
-   [Llama-2-13B-chat locally on your M1/M2 Mac with GPU inference](https://gist.github.com/adrienbrault/b76631c56c736def9bc1bc2167b5d129)

## Other Sources

-   [Llama 2 Resources](https://gpus.llm-utils.org/llama-2-resources/)

Let me know if you would like me to expand on any section or add additional details. I aimed to provide a high-level overview of key information related to LLaMA 2's release based on what is publicly known so far.