{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 扩展说明: 指令微调 Llama 2\n",
    "\n",
    "这篇博客是一篇来自 Meta AI，关于指令微调 Llama 2 的扩展说明。旨在聚焦构建指令数据集，有了它，我们则可以使用自己的指令来微调 Llama 2 基础模型。\n",
    "\n",
    "目标是构建一个能够基于输入内容来生成指令的模型。这么做背后的逻辑是，模型如此就可以由其他人生成自己的指令数据集。这在当想开发私人个性化定制模型，如发送推特、写邮件等，时很方便。这也意味着你可以通过你的邮件来生成一个指令数据集，然后用它来训练一个模型来为你写邮件。\n",
    "\n",
    "好，那我们来开始吧？我们将进行:\n",
    "\n",
    "1. 定义应用场景细节并创建指令的提示词模板\n",
    "2. 构建指令数据集\n",
    "3. 使用 `trl` 与 `SFTTrainer` 指令微调 Llama 2\n",
    "4. 测试模型、进行推理\n",
    "\n",
    "## 1. 定义应用场景细节并创建指令的提示词模板\n",
    "\n",
    "在描述应用场景前，我们要更好的理解一下究竟什么是指令。\n",
    "\n",
    "> 指令是一段文本或提供给大语言模型，类似 Llama，GPT-4 或 Claude，使用的提示词，用来指导它去生成回复。指令可以让人们做到把控对话，约束模型输出更自然、实用的输出，并使这些结果能够对齐用户的目的。制作清晰的、整洁的指令则是生成高质量对话的关键。\n",
    "\n",
    "指令的例子如下表所示。\n",
    "\n",
    "| 能力 | 示例指令 |\n",
    "| --- | --- |\n",
    "| 头脑风暴 | 提供一系列新口味的冰淇淋的创意。 |\n",
    "| 分类 | 根据剧情概要，将这些电影归类为喜剧、戏剧或恐怖片。 |\n",
    "| 确定性问答 | 用一个单词回答“法国的首都是哪里？” |\n",
    "| 生成 | 用罗伯特·弗罗斯特的风格写一首关于大自然和季节变化的诗。 |\n",
    "| 信息提取 | 从这篇短文中提取主要人物的名字。 |\n",
    "| 开放性问答 | 为什么树叶在秋天会变色？用科学的理由解释一下。 |\n",
    "| 摘要 | 用 2-3 句话概括一下这篇关于可再生能源最新进展的文章。 |\n",
    "\n",
    "如开头所述，我们想要微调模型，以便根据输入 (或输出) 生成指令。 我们希望将其用作创建合成数据集的方法，以赋予 LLM 和代理个性化能力。\n",
    "\n",
    "把这个想法转换成一个基础的提示模板，按照 [Alpaca 格式](https://github.com/tatsu-lab/stanford_alpaca#data-release).\n",
    "\n",
    "```python\n",
    "### Instruction:\n",
    "Use the Input below to create an instruction, which could have been used to generate the input using an LLM. \n",
    "\n",
    "### Input:\n",
    "Dear [boss name],\n",
    "\n",
    "I'm writing to request next week, August 1st through August 4th,\n",
    "off as paid time off.\n",
    "\n",
    "I have some personal matters to attend to that week that require \n",
    "me to be out of the office. I wanted to give you as much advance \n",
    "notice as possible so you can plan accordingly while I am away.\n",
    "\n",
    "Please let me know if you need any additional information from me \n",
    "or have any concerns with me taking next week off. I appreciate you \n",
    "considering this request.\n",
    "\n",
    "Thank you, [Your name]\n",
    "\n",
    "### Response:\n",
    "Write an email to my boss that I need next week 08/01 - 08/04 off.\n",
    "```\n",
    "\n",
    "## 2. 创建指令数据集\n",
    "\n",
    "在定义了我们的应用场景和提示模板后，我们需要创建自己的指令数据集。创建高质量的指令数据集是获得良好模型性能的关键。研究表明，[“对齐，越少越好”](https://arxiv.org/abs/2305.11206) 表明，创建高质量、低数量 (大约 1000 个样本) 的数据集可以达到与低质量、高数量的数据集相同的性能。\n",
    "\n",
    "创建指令数据集有几种方法，包括:\n",
    "\n",
    "1. 使用现有数据集并将其转换为指令数据集，例如 [FLAN](https://huggingface.co/datasets/SirNeural/flan_v2)\n",
    "2. 使用现有的 LLM 创建合成指令数据集，例如 [Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca)\n",
    "3. 人力创建指令数据集，例如 [Dolly](https://huggingface.co/datasets/databricks/databricks-dolly-15k)。\n",
    "\n",
    "每种方法都有其优缺点，这取决于预算、时间和质量要求。例如，使用现有数据集是最简单的，但可能不适合您的特定用例，而使用人力可能是最准确的，但必然耗时、昂贵。也可以结合几种不同方法来创建指令数据集，如 [Orca: Progressive Learning from Complex Explanation Traces of GPT-4.](https://arxiv.org/abs/2306.02707)。\n",
    "\n",
    "为了简单起见，我们将使用 **[Dolly](https://huggingface.co/datasets/databricks/databricks-dolly-15k)**，这是一个开源的指令跟踪记录数据集，由数千名 Databricks 员工在 **[InstructGPT paper](https://arxiv.org/abs/2203.02155)** 中描述的几个行为类别中生成，包括头脑风暴、分类、确定性回答、生成、信息提取、开放性回答和摘要。\n",
    "\n",
    "开始编程吧，首先，我们来安装依赖项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"transformers==4.31.0\" \"datasets==2.13.0\" \"peft==0.4.0\" \"accelerate==0.21.0\" \"bitsandbytes==0.40.2\" \"trl==0.4.7\" \"safetensors>=0.3.1\" --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用 🤗 Datasets library 的 **`load_dataset()`** 方法加载 **`databricks/databricks-dolly-15k`** 数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# 从hub加载数据集\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "print(dataset[randrange(len(dataset))])\n",
    "# dataset size: 15011"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了指导我们的模型，我们需要将我们的结构化示例转换为通过指令描述的任务集合。我们定义一个 **`formatting_function`** ，它接受一个样本并返回一个符合格式指令的字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(sample):\n",
    "\treturn f\"\"\"### Instruction:\n",
    "Use the Input below to create an instruction, which could have been used to generate the input using an LLM. \n",
    "\n",
    "### Input:\n",
    "{sample['response']}\n",
    "\n",
    "### Response:\n",
    "{sample['instruction']}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来在一个随机的例子上测试一下我们的结构化函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_instruction(dataset[randrange(len(dataset))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 使用 `trl` 和`SFTTrainer` 指令微调 Llama 2\n",
    "\n",
    "我们将使用最近在由 Tim Dettmers 等人的发表的论文“[QLoRA: Quantization-aware Low-Rank Adapter Tuning for Language Generation](https://arxiv.org/abs/2305.14314)”中介绍的方法。QLoRA 是一种新的技术，用于在微调期间减少大型语言模型的内存占用，且并不会降低性能。QLoRA 的 TL;DR; 是这样工作的:\n",
    "\n",
    "- 将预训练模型量化为 4bit 位并冻结它。\n",
    "- 附加轻量化的、可训练的适配器层。(LoRA)\n",
    "- 在使用冻结的量化模型基于文本内容进行微调时，仅微调适配器层参数。\n",
    "\n",
    "如果您想了解有关 QLoRA 及其工作原理的更多信息，我建议您阅读 **[Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes)** 博客文章。\n",
    "\n",
    "### Flash Attention (快速注意力)\n",
    "\n",
    "Flash Attention 是一种经过重新排序的注意力计算方法，它利用经典技术 (排列、重计算) 来显著加快速度，将序列长度的内存使用量从二次降低到线性。它基于论文“[FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)”。\n",
    "\n",
    "TL;DR; 将训练加速了 3 倍。在这儿获得更多信息 [FlashAttention](https://github.com/Dao-AILab/flash-attention/tree/main)。 Flash Attention 目前仅支持 Ampere (A10, A40, A100, …) & Hopper (H100, …) GPU。 你可以检查一下你的 GPU 是否支持，并用下面的命令来安装它:\n",
    "\n",
    "注意: 如果您的机器的内存小于 96GB，而 CPU 核心数足够多，请减少 `MAX_JOBS` 的数量。在我们使用的 `g5.2xlarge` 上，我们使用了 `4` 。\n",
    "\n",
    "```bash\n",
    "python -c \"import torch; assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported for Flash Attention'\"\n",
    "pip install ninja packaging\n",
    "MAX_JOBS=4 pip install flash-attn --no-build-isolation\n",
    "```\n",
    "\n",
    "_安装 flash attention 是会需要一些时间 (10-45 分钟)_。\n",
    "\n",
    "该示例支持对所有 Llama 检查点使用 Flash Attention，但默认是未启用的。要开启 Flash Attention，请取消代码块中这段的注释， `# COMMENT IN TO USE FLASH ATTENTION` 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "use_flash_attention = False\n",
    "\n",
    "# COMMENT IN TO USE FLASH ATTENTION\n",
    "# replace attention with flash attention \n",
    "# if torch.cuda.get_device_capability()[0] >= 8:\n",
    "#     from utils.llama_patch import replace_attn_with_flash_attn\n",
    "#     print(\"Using flash attention\")\n",
    "#     replace_attn_with_flash_attn()\n",
    "#     use_flash_attention = True\n",
    "\n",
    "\n",
    "# Hugging Face 模型id\n",
    "model_id = \"NousResearch/Llama-2-7b-hf\" # non-gated\n",
    "# model_id = \"meta-llama/Llama-2-7b-hf\" # gated\n",
    "\n",
    "\n",
    "# BitsAndBytesConfig int-4 config \n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# 加载模型与分词器\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, use_cache=False, device_map=\"auto\")\n",
    "model.config.pretraining_tp = 1 \n",
    "\n",
    "# 通过对比doc中的字符串，验证模型是在使用flash attention\n",
    "if use_flash_attention:\n",
    "    from utils.llama_patch import forward    \n",
    "    assert model.model.layers[0].self_attn.forward.__doc__ == forward.__doc__, \"Model is not using flash attention\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`SFTTrainer`** 支持与 **`peft`** 的本地集成，这使得高效地指令微调LLM变得非常容易。我们只需要创建 **`LoRAConfig`** 并将其提供给训练器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "# 基于 QLoRA 论文来配置 LoRA\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        r=64,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\", \n",
    ")\n",
    "\n",
    "\n",
    "# 为训练准备好模型\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在开始训练之前，我们需要定义自己想要的超参数 (`TrainingArguments`)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"llama-7-int4-dolly\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=6 if use_flash_attention else 4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    disable_tqdm=True # 当配置的参数都正确后可以关闭tqdm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在有了用来训练模型 `SFTTrainer` 所需要准备的每一个模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 2048 # 数据集的最大长度序列\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    formatting_func=format_instruction, \n",
    "    args=args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过调用 `Trainer` 实例上的 `train()` 方法来训练我们的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "trainer.train() # tqdm关闭后将不显示进度条信息\n",
    "\n",
    "# 保存模型\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不使用 Flash Attention 的训练过程在 `g5.2xlarge` 上花费了 03:08:00。实例的成本为 `1,212$/h` ，总成本为 `3.7$` 。\n",
    "\n",
    "使用 Flash Attention 的训练过程在 `g5.2xlarge` 上花费了 02:08:00。实例的成本为 `1,212$/h` ，总成本为 `2.6$` 。\n",
    "\n",
    "使用 Flash Attention 的结果令人满意，速度提高了 1.5 倍，成本降低了 30%。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 测试模型、进行推理\n",
    "\n",
    "在训练完成后，我们想要运行和测试模型。我们会使用 `peft` 和 `transformers` 将 LoRA 适配器加载到模型中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_flash_attention:\n",
    "    # 停止 flash attention\n",
    "    from utils.llama_patch import unplace_flash_attn_with_attn\n",
    "    unplace_flash_attn_with_attn()\n",
    "    \n",
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "args.output_dir = \"llama-7-int4-dolly\"\n",
    "\n",
    "# 加载基础LLM模型与分词器\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    args.output_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ") \n",
    "tokenizer = AutoTokenizer.from_pretrained(args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来再次用随机样本加载一次数据集，试着来生成一条指令。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset \n",
    "from random import randrange\n",
    "\n",
    "\n",
    "# 从hub加载数据集并得到一个样本\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "sample = dataset[randrange(len(dataset))]\n",
    "\n",
    "prompt = f\"\"\"### Instruction:\n",
    "Use the Input below to create an instruction, which could have been used to generate the input using an LLM. \n",
    "\n",
    "### Input:\n",
    "{sample['response']}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "# with torch.inference_mode():\n",
    "outputs = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.9)\n",
    "\n",
    "print(f\"Prompt:\\n{sample['response']}\\n\")\n",
    "print(f\"Generated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\")\n",
    "print(f\"Ground truth:\\n{sample['instruction']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "太好了！我们的模型可以工作了！如果想要加速我们的模型，我们可以使用 [Text Generation Inference](https://github.com/huggingface/text-generation-inference) 部署它。因此我们需要将我们适配器的参数合并到基础模型中去。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    args.output_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    ") \n",
    "\n",
    "# 合并 LoRA 与 base model\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# 保存合并后的模型\n",
    "merged_model.save_pretrained(\"merged_model\",safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"merged_model\")\n",
    "\n",
    "# push合并的模型到hub上\n",
    "# merged_model.push_to_hub(\"user/repo\")\n",
    "# tokenizer.push_to_hub(\"user/repo\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
