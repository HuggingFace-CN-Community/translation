{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "id": "CYiiLlLuGRSd"
      },
      "source": [
        "---\n",
        "date: \"01/07/2024\"\n",
        "twitter-card: true\n",
        "title: 句子嵌入\n",
        "description: 这里有你想知道的关于句子嵌入的一切（未来会加入更多相关的内容）\n",
        "image: embedding.png\n",
        "toc-depth: 5\n",
        "format:\n",
        "  html:\n",
        "    comments:\n",
        "      utterances:\n",
        "         repo: osanseviero/hackerllama\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xG-PipBiGRSd"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/osanseviero/hackerllama/blob/main/nbs/blog/posts/sentence_embeddings/index.ipynb\" rel=\"nofollow\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJqGmYSQGRSe"
      },
      "source": [
        "这个系列旨在揭示嵌入（embeddings）的奥秘，并向您展示如何在项目中使用它们。本系列的第一篇博客将教您如何使用和扩展开源嵌入模型。我们将探讨选择现有模型的标准、当前的评估方法以及生态系统的现状。我们还将深入研究三个令人兴奋的技术实践案例：\n",
        "\n",
        "\n",
        "* 寻找最相似的Quora或StackOverflow问题（注：Quora为美国一个著名的问答网站，Stack Overflow是一个美国的程序员问答网站）\n",
        "* 在给定的大型数据集中，找到最相似的项\n",
        "* 在用户浏览器中直接运行搜索嵌入模型（无需服务器）\n",
        "\n",
        "您可以在这里阅读内容，或通过点击页面顶部的 Google Colab 徽章在 Google Colab 中执行它。让我们深入了解嵌入！"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZW89x_3GRSf"
      },
      "source": [
        "## 摘要\n",
        "\n",
        "也许你一直在阅读关于“嵌入这个”和“嵌入那个”的内容，但你可能仍然不完全了解嵌入究竟是什么。你并不孤单！即使你对嵌入只有一个模糊的概念，你仍可通过一个黑匣子 API（注：因为训练的模型，我们并不知道模型工作时内部信息传递方式是如何的，因此称作黑盒）去使用模型，而你并不知道模型内部发生了什么。这是一个问题，因为当前开源的嵌入模型的性能非常强大-它们容易不但容易部署，而且体积小（因此托管成本较低），并且胜过许多闭源模型。\n",
        "\n",
        "嵌入将信息表现为一组数字（名为向量）（可以将其视为列表！）。例如，我们可以获得单词、句子、文档、图像、音频文件等的值。给定句子“今天是个晴天”，我们可以获得其向量值，它将是一个特定大小的向量，例如384个数字（这样的向量可能看起来像 [0.32, 0.42, 0.15, …, 0.72]）。有趣的是，嵌入捕捉到了信息的语义含义。例如，句子“今天是个晴天”和句子“今天天气不错”非常相似。即使单词不同，含义相似，嵌入的向量值也会反映出这一点。\n",
        "\n",
        ":::callout\n",
        "\n",
        "如果你不确定诸如“向量”、“语义相似性”、“向量大小”或“预训练”等术语的含义，不用担心！我们将在接下来的部分对它们进行逐一解释。我们首先要先专注于浅层次的理解。\n",
        "\n",
        ":::\n",
        "\n",
        "因此，这个向量捕捉到了信息的语义含义，使得它们更容易相互比较。例如，我们可以使用向量来在Quora或StackOverflow中找到相似的问题，搜索代码，找到相似的图像等。让我们深入了解一些代码！"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1diSV7DtGRSf"
      },
      "source": [
        "我们将使用Sentence Transformers，这是一个开源的第三方python库，它使得预训练嵌入模型的使用变得非常简单。具体来说，Sentence Transformers允许我们快速将句子由自然语言转换为向量。让我们运行一个例子，然后讨论它在幕后是如何工作的。\n",
        "\n",
        "让我们开始安装这个库："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XA-t5v9HGRSg",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px-H9e9bGRSg"
      },
      "source": [
        "第二步是加载一个已存在的模型。我们将开始使用 [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)。虽然它不是最好的开源向量模型，但它相当受用户欢迎且参数量非常小（2300万参数），这意味着我们可以非常快速地开始使用它。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-ffoVWCGRSh",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyznQwIuGRSh"
      },
      "source": [
        "既然我们已经加载了一个模型，让我们使用它来对一些句子进行编码。我们可以使用`encode`方法来获得一个句子列表的向量值。让我们试一试吧！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yd7t_PsMGRSh",
        "outputId": "e0c125cf-a9c6-4ce4-ef85-194deb4daeaa",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3, 384)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sentence_transformers import util\n",
        "\n",
        "sentences = [\"The weather today is beautiful\", \"It's raining!\", \"Dogs are awesome\"]\n",
        "embeddings = model.encode(sentences)\n",
        "embeddings.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBUyBPrnGRSi"
      },
      "source": [
        "all-MiniLM-L6-v2创建了包含384个向量值的嵌入。我们获得了三个向量，每个向量对应一句话。将`embeddings`视为嵌入的“数据库”。给定一个新的句子，我们如何找到最相似的句子呢？我们可以使用`util.pytorch_cos_sim`方法计算新句子的向量值与数据库中所有向量值之间的余弦相似度（我们很快会详细讨论它）。余弦相似度是一个介于0和1之间的数字，表示两个向量有多相似。值为1意味着向量是相同的，而值为0意味着向量是完全不同的。让我们试一试吧！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMvMOZGCGRSi",
        "outputId": "0df23613-8b90-4491-aad7-968654c9c309",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.7344]]) The weather today is beautiful\n",
            "tensor([[0.4180]]) It's raining!\n",
            "tensor([[0.1060]]) Dogs are awesome\n"
          ]
        }
      ],
      "source": [
        "first_embedding = model.encode(\"Today is a sunny day\")\n",
        "for embedding, sentence in zip(embeddings, sentences):\n",
        "    similarity = util.pytorch_cos_sim(first_embedding, embedding)\n",
        "    print(similarity, sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvYytBKTGRSi"
      },
      "source": [
        "我们对此可以得出什么解释呢？尽管“today is a sunny day”和“the weather today is beautiful”没有相同的词，但嵌入可以捕捉到一些语义含义，因此余弦相似度相对较高。另一方面，“Dogs are awesome”，虽然正确，但与天气或今天无关；因此，余弦相似度非常低。\n",
        "\n",
        "为了深入探讨相似嵌入的这个概念，让我们看看它们如何在产品中使用。想象一下，美国社会保障局希望允许用户在输入框中提出与医疗保险相关的问题。这个话题非常敏感，我们可能不希望模型产生与此无关的幻觉！相反，我们可以利用一个问题数据库（在这种情况下，存在一个现有的医疗保险常见问题解答）。这个过程与上面类似：\n",
        "\n",
        "1. 我们有一个问题和答案的语料库（集合）。\n",
        "2. 我们计算所有问题的向量值。\n",
        "3. 给定一个新问题，我们计算它的向量值。\n",
        "4. 计算新问题向量与数据库中所有向量之间的余弦相似度。\n",
        "5. 我们返回与最相似嵌入相关联的最相似的问题。\n",
        "\n",
        "步骤1和2可以在离线状态下完成（即，我们仅计算一次嵌入并存储它们）。其余步骤可以在搜索时进行（每当用户提出一个问题时）。让我们看看这在代码中是什么样子的。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDrzgLvwGRSi"
      },
      "source": [
        "[![Representation of embeddings in two dimensions](https://github.com/osanseviero/hackerllama/blob/main/nbs/blog/posts/sentence_embeddings/embedding.png?raw=1)](https://huggingface.co/spaces/sentence-transformers/embeddings-semantic-search)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlqP-_k3GRSi"
      },
      "source": [
        "让我们先创建我们的常见问题的映射"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESJ9EFBQGRSj",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Data from https://faq.ssa.gov/en-US/topic/?id=CAT-01092\n",
        "\n",
        "faq = {\n",
        "    \"How do I get a replacement Medicare card?\": \"If your Medicare card was lost, stolen, or destroyed, you can request a replacement online at Medicare.gov.\",\n",
        "    \"How do I sign up for Medicare?\": \"If you already get Social Security benefits, you do not need to sign up for Medicare. We will automatically enroll you in Original Medicare (Part A and Part B) when you become eligible. We will mail you the information a few months before you become eligible.\",\n",
        "    \"What are Medicare late enrollment penalties?\": \"In most cases, if you don’t sign up for Medicare when you’re first eligible, you may have to pay a higher monthly premium. Find more information at https://faq.ssa.gov/en-us/Topic/article/KA-02995\",\n",
        "    \"Will my Medicare premiums be higher because of my higher income?\": \"Some people with higher income may pay a larger percentage of their monthly Medicare Part B and prescription drug costs based on their income. We call the additional amount the income-related monthly adjustment amount.\",\n",
        "    \"What is Medicare and who can get it?\": \"Medicare is a health insurance program for people age 65 or older. Some younger people are eligible for Medicare including people with disabilities, permanent kidney failure and amyotrophic lateral sclerosis (Lou Gehrig’s disease or ALS). Medicare helps with the cost of health care, but it does not cover all medical expenses or the cost of most long-term care.\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9gyb2PXGRSj"
      },
      "source": [
        "然后，我们使用`encode`方法来获取所有问题的嵌入。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6ygAkknGRSj",
        "outputId": "cdf5cf25-f157-4834-9267-514e00de00b2",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(5, 384)\n"
          ]
        }
      ],
      "source": [
        "corpus_embeddings = model.encode(list(faq.keys()))\n",
        "print(corpus_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7Rx4SgRGRSj"
      },
      "source": [
        "一旦用户提出一个问题，我们就会获取它的嵌入。通常我们将这个嵌入称为查询嵌入。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deqffHqFGRSj",
        "outputId": "77a51589-470a-4acf-dffe-ab1064800ff3",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(384,)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "user_question = \"Do I need to pay more after a raise?\"\n",
        "query_embedding = model.encode(user_question)\n",
        "query_embedding.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weT5FBvkGRSj"
      },
      "source": [
        "现在我们可以计算语料库嵌入和查询嵌入之间的相似度。我们可以使用循环并像之前一样使用`util.pytorch.cos_sim`，但是Sentence Transformers提供了一个更友好的方法叫做`semantic_search`，它为我们完成了所有的工作。它返回前k个最相似的嵌入及它们的相似度分数。让我们试试吧！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZkAD9SoGRSj",
        "outputId": "2b25d1c5-ea1e-4283-d53a-7e4b4043e712",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[{'corpus_id': 3, 'score': 0.35796287655830383},\n",
              "  {'corpus_id': 2, 'score': 0.2787758708000183},\n",
              "  {'corpus_id': 1, 'score': 0.15840476751327515}]]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "similarities = util.semantic_search(query_embedding, corpus_embeddings, top_k=3)\n",
        "similarities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2spfgiAMGRSj"
      },
      "source": [
        "现在让我们看看这对应哪些问题和答案："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iCeaay5GRSj",
        "outputId": "edce88d7-4f2f-4f37-963a-386243bfbc0b",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 1 question (p=0.35796287655830383): Will my Medicare premiums be higher because of my higher income?\n",
            "Answer: Some people with higher income may pay a larger percentage of their monthly Medicare Part B and prescription drug costs based on their income. We call the additional amount the income-related monthly adjustment amount.\n",
            "Top 2 question (p=0.2787758708000183): What are Medicare late enrollment penalties?\n",
            "Answer: In most cases, if you don’t sign up for Medicare when you’re first eligible, you may have to pay a higher monthly premium. Find more information at https://faq.ssa.gov/en-us/Topic/article/KA-02995\n",
            "Top 3 question (p=0.15840476751327515): How do I sign up for Medicare?\n",
            "Answer: If you already get Social Security benefits, you do not need to sign up for Medicare. We will automatically enroll you in Original Medicare (Part A and Part B) when you become eligible. We will mail you the information a few months before you become eligible.\n"
          ]
        }
      ],
      "source": [
        "for i, result in enumerate(similarities[0]):\n",
        "    corpus_id = result[\"corpus_id\"]\n",
        "    score = result[\"score\"]\n",
        "    print(f\"Top {i+1} question (p={score}): {list(faq.keys())[corpus_id]}\")\n",
        "    print(f\"Answer: {list(faq.values())[corpus_id]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jXgCcuYGRSk"
      },
      "source": [
        "太好了，所以针对问题\"Do I need to pay more after a raise?\"，我们知道最相似的问题是\"Will my Medicare premiums be higher because of my higher income?\"，因此我们可以返回提供的答案。在实践中，你可能会有数千到数百万个嵌入，但这是一个简单而又强大的例子，展示了如何使用嵌入来找到相似的问题。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0f6hveUGRSk"
      },
      "source": [
        "既然我们更好地理解了嵌入是什么以及它们如何被使用，让我们深入研究一下它们吧！"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRh1ZUYRGRSk"
      },
      "source": [
        "## 从单词嵌入到句子嵌入\n",
        "\n",
        "### Word2Vec和GloVe\n",
        "\n",
        "现在是时候退后一步，了解更多关于嵌入以及它们为何被需要的知识了。像BERT这样的神经网络不能直接处理单词；它们需要数字。而提供单词的方法是将它们表示为向量，也称为单词嵌入。\n",
        "\n",
        "在传统的设置中，您定义了一个词汇表（指定了允许的单词），然后该词汇表中的每个单词都有一个分配的嵌入。不在词汇表中的单词被映射到一个特殊的标记，通常称为<UNK>（表示训练期间未找到的标准占位符单词）。例如，假设我们有一个包含三个单词的词汇表，并且我们为每个单词分配了一个大小为五的向量。我们可能有以下嵌入：\n",
        "\n",
        "| Word | Embedding |\n",
        "| ---- | --------- |\n",
        "| king    | [0.15, 0.2, 0.2, 0.3, 0.5] |\n",
        "| queen   | [0.12, 0.1, 0.19, 0.3, 0.47] |\n",
        "| potato    | [0.13, 0.4, 0.1, 0.15, 0.01] |\n",
        "| `<UNK>`    | [0.01, 0.02, 0.01, 0.4, 0.11] |\n",
        "\n",
        "我上面写的嵌入是我随机写的数字。实际上，**这些嵌入是通过学习得到的**。这是诸如[Word2Vec](https://en.wikipedia.org/wiki/Word2vec)和[GloVe](https://nlp.stanford.edu/pubs/glove.pdf)等方法的主要思想。它们以这样一种方式学习语料库中单词的嵌入，即出现在相似上下文中的单词具有相似的嵌入。例如，\"king\"和\"queen\"的嵌入是相似的，因为它们出现在相似的上下文中。\n",
        "\n",
        "[![Word embeddings](https://github.com/osanseviero/hackerllama/blob/main/nbs/blog/posts/sentence_embeddings/embedding.png?raw=1)](https://huggingface.co/spaces/sentence-transformers/embeddings-semantic-search)\n",
        "\n",
        "一些开源库，如Gensim和fastText，允许您快速获取预训练的Word2Vec和GloVe嵌入。在NLP的“黄金时代”（2013年），人们使用这些模型来计算单词嵌入，这对于其他模型的输入非常有帮助。例如，您可以计算句子中每个单词的单词嵌入，然后将其作为输入传递给scikit-learn分类器，以对句子的情感进行分类。\n",
        "\n",
        "GloVe和Word2Vec具有固定的表示形式。一旦它们被训练，每个单词都被分配一个固定的向量表示，不考虑它们的上下文（因此，“river bank”中的“bank”和“savings bank”中的“bank”将具有相同的嵌入）。**Word2Vec和GloVe在处理具有多个含义的单词时会遇到困难。**\n",
        "\n",
        "![The good ol' days of NLP](https://github.com/osanseviero/hackerllama/blob/main/nbs/blog/posts/sentence_embeddings/word2vec_meme.jpeg?raw=1)\n",
        "\n",
        "\n",
        ":::注意\n",
        "\n",
        "了解word2vec和GloVe的细节对于理解博客文章的其余部分和句子嵌入是不必要的，所以我会跳过它们。如果你感兴趣的话，我建议阅读这门出色的交互式NLP课程中的这一章节。\n",
        "\n",
        "长话短说\n",
        "\n",
        "* Word2Vec 是通过传递一个非常大的语料库并训练一个浅层神经网络来预测周围的单词来进行训练的。后来的替代方法则是根据周围的单词来预测中心单词。\n",
        "* GloVe 是通过查看单词的共现矩阵（单词在一定距离内一起出现的频率）进行训练的，然后使用该矩阵来获取嵌入。\n",
        "\n",
        "Word2Vec 和 GloVe 的训练目标确保出现在相似上下文中的单词具有相似的嵌入。\n",
        "\n",
        ":::"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jQ3XqV2GRSk"
      },
      "source": [
        "### 使用Transformers的词嵌入\n",
        "\n",
        "最近，随着Transformer的出现，我们有了新的计算嵌入的方式。嵌入也是通过学习获得的，但与训练一个嵌入模型，然后再为特定任务训练另一个模型不同，Transformer在其任务的上下文中学习有用的嵌入。例如，BERT，一个流行的Transformer模型，在掩码语言建模（预测哪个单词填在空白处）和下一个句子预测（句子B是否跟在句子A后面）的上下文中学习单词嵌入。\n",
        "\n",
        "Transformer在许多NLP任务中是最先进的，并且能够捕捉word2vec和GloVe无法捕捉到的上下文信息，这要归功于一种称为注意力的机制。注意力允许模型权衡其他单词的重要性并捕捉上下文信息。例如，在句子“I went to the bank to deposit money”中，单词“bank”是含糊的。它是河岸还是储蓄银行？模型可以使用单词“deposit”来理解它是一个储蓄银行。这些是**上下文化的嵌入** - 它们的单词嵌入可以根据周围的单词而不同。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGEhklUzGRSk"
      },
      "source": [
        "好的...我们谈了很多关于词嵌入的内容；是时候运行一些代码了。让我们使用一个预训练的Transformer模型，[bert-base-uncased](https://huggingface.co/bert-base-uncased)，并获取一些单词嵌入。我们将使用`transformers`库来实现这一点。让我们首先加载模型及其分词器。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8K4o7JvGRSk",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIAZp60aGRSk"
      },
      "source": [
        "到目前为止，我们还没有讨论过分词(tokenization)。直到现在，我们假设我们将数据分割成单词。当使用transformers时，我们将文本分割为标记(tokens)。例如，单词“banking”可以被分成两个标记，“bank”和“ing”。分词器负责将数据分割成标记，它分割数据的方式是模型特定的，并且是一个确定性学习过程，这意味着相同的单词将始终被分割成相同的标记。让我们看看代码中是什么样子的："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRjG9Q4SGRSk",
        "outputId": "081d7dc3-6f29-42da-b969-5ba3f043751e",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['[CLS]', 'the', 'king', 'and', 'the', 'queen', 'are', 'happy', '.', '[SEP]']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"The king and the queen are happy.\"\n",
        "tokenizer.tokenize(text, add_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPWHXW3UGRSl"
      },
      "source": [
        "好的，在这个例子中，每个单词都是一个标记！（但这并不总是如此，我们很快就会看到）。但我们还看到了两件可能意外的事情：`[CLS]` 和 `[SEP]`。这些是添加到句子开头和结尾的特殊标记。这些是因为BERT是以这种格式进行训练的。BERT的训练目标之一是下一个句子预测，这意味着它被训练来预测两个句子是否是连续的。`[CLS]` 标记表示整个句子，而 `[SEP]` 标记分隔句子。当我们谈论句子嵌入时，这将会很有趣。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pn9oHTa4GRSl"
      },
      "source": [
        " 现在让我们获取每个标记的嵌入。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yf_PrFh5GRSm",
        "outputId": "6940616a-7b2a-42b2-feb3-d6879deba438",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 10, 768])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoded_input = tokenizer(text, return_tensors=\"pt\")\n",
        "output = model(**encoded_input)\n",
        "output[\"last_hidden_state\"].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezTKr0K3GRSm"
      },
      "source": [
        "太好了！BERT为我们提供了每个标记的768个值的嵌入。这些标记都包含语义信息 - **它们捕捉了单词在句子上下文中的含义**。让我们看看在这个上下文中对应于单词\"king\"的嵌入是否与\"queen\"的嵌入相似。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjfTHYwUGRSm",
        "outputId": "766f1741-1cdf-4e83-9967-20b5d595f8bc",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of embedding torch.Size([768])\n",
            "Similarity between king and queen embedding 0.7920711040496826\n"
          ]
        }
      ],
      "source": [
        "king_embedding = output[\"last_hidden_state\"][0][2]  # 2 is the position of king\n",
        "queen_embedding = output[\"last_hidden_state\"][0][5]  # 5 is the position of queen\n",
        "print(f\"Shape of embedding {king_embedding.shape}\")\n",
        "print(\n",
        "    f\"Similarity between king and queen embedding {util.pytorch_cos_sim(king_embedding, queen_embedding)[0][0]}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noR5bXQ8GRSm"
      },
      "source": [
        "好的，在这个上下文中它们看起来是相当相似的！现在让我们看一下单词\"happy\"。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UovJ8jhpGRSm",
        "outputId": "d1985f35-b4e4-49c2-b293-b624ea9a31c6",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.5239]], grad_fn=<MmBackward0>)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "happy_embedding = output.last_hidden_state[0][7]  # happy\n",
        "util.pytorch_cos_sim(king_embedding, happy_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC1l-aNAGRSq"
      },
      "source": [
        "这是合理的；皇后的嵌入与国王的更相似，而不是快乐的嵌入。\n",
        "\n",
        "现在让我们看看同一个单词在不同的上下文中可能具有不同的值："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFEX_8FHGRSq",
        "outputId": "7319cab9-666c-43f0-8de0-c77eee5334c0",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 7, 768])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"The angry and unhappy king\"\n",
        "encoded_input = tokenizer(text, return_tensors=\"pt\")\n",
        "output = model(**encoded_input)\n",
        "output[\"last_hidden_state\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCnUCH-jGRSq",
        "outputId": "6266715d-7e61-48b6-999e-4d6bc2e981d5",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['[CLS]', 'the', 'angry', 'and', 'unhappy', 'king', '[SEP]']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.tokenize(text, add_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxjAJgF-GRSq",
        "outputId": "67f541f1-1703-4346-d64b-d431a552028c",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.5740]], grad_fn=<MmBackward0>)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "king_embedding_2 = output[\"last_hidden_state\"][0][5]\n",
        "util.pytorch_cos_sim(king_embedding, king_embedding_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6qkGjmVGRSq"
      },
      "source": [
        "哇！尽管两个嵌入似乎对应于\"king\"的嵌入，但它们在向量空间中相差很大。发生了什么？请记住，这些是上下文化嵌入。第一个句子的上下文非常积极，而第二个句子的上下文则相当消极。因此，这些嵌入是不同的。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtOniTxtGRSq"
      },
      "source": [
        "之前，我们讨论了分词器如何将一个单词分成多个标记的情况。一个合理的问题是在这种情况下我们如何获得单词嵌入。让我们来看一个例子，使用长单词“tokenization”。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YesiW-fGRSq",
        "outputId": "3a1e9dc1-54ce-4529-9a42-d2218ed75fe2",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['token', '##ization']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.tokenize(\"tokenization\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQKR2TM3GRSq"
      },
      "source": [
        "单词\"tokenization\"被分成了两个标记，但我们关心的是\"tokenization\"的嵌入！我们可以采取一种汇聚策略，其中我们获取每个标记的嵌入，然后对它们进行平均以获得单词嵌入。让我们试试看！\n",
        "\n",
        "和之前一样，我们首先对文本进行分词，然后将标记ID传递给模型。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gx-hFjXIGRSq",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "text = \"this is about tokenization\"\n",
        "\n",
        "encoded_input = tokenizer(text, return_tensors=\"pt\")\n",
        "output = model(**encoded_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KkuT8ETGRSq"
      },
      "source": [
        "让我们看一下对句子进行的分词处理："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzCoO1lVGRSr",
        "outputId": "a200d51b-acf7-4746-e5ea-46d5732718d4",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['[CLS]', 'this', 'is', 'about', 'token', '##ization', '[SEP]']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.tokenize(text, add_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWaRu7JpGRSr"
      },
      "source": [
        "所以我们想要通过对标记4和5的嵌入进行平均来汇集它们的嵌入。让我们首先获取这些标记的嵌入。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAUn0dI8GRSr",
        "outputId": "48d7559b-3e52-4005-ab1a-a183bc9e641a",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 768])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_token_indices = [4, 5]\n",
        "word_embeddings = output[\"last_hidden_state\"][0, word_token_indices]\n",
        "word_embeddings.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMS8NqvVGRSr"
      },
      "source": [
        "现在让我们使用`torch.mean`对它们进行平均。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CC6HaShzGRSr",
        "outputId": "e995be3d-7743-4fa8-c195-7fadcdc844a0",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([768])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "torch.mean(word_embeddings, dim=0).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XaSIPx2GRSr"
      },
      "source": [
        "让我们将所有这些内容封装到一个函数中，这样我们以后就可以轻松使用它。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cckTvuE3GRSr",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def get_word_embedding(text, word):\n",
        "    # Encode the text and do a forward pass through the model to get the hidden states\n",
        "    encoded_input = tokenizer(text, return_tensors=\"pt\")\n",
        "    with torch.no_grad():  # We don't need gradients for embedding extraction\n",
        "        output = model(**encoded_input)\n",
        "\n",
        "    # Find the indices for the word\n",
        "    word_ids = tokenizer.encode(\n",
        "        word, add_special_tokens=False\n",
        "    )  # No special tokens anymore\n",
        "    word_token_indices = [\n",
        "        i\n",
        "        for i, token_id in enumerate(encoded_input[\"input_ids\"][0])\n",
        "        if token_id in word_ids\n",
        "    ]\n",
        "\n",
        "    # Pool the embeddings for the word\n",
        "    word_embeddings = output[\"last_hidden_state\"][0, word_token_indices]\n",
        "    return torch.mean(word_embeddings, dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9ICAYNXGRSr"
      },
      "source": [
        "**例子1.** 在两者都生气的情况下，\"king\"和\"queen\"嵌入之间的相似度。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TmL4hbUGRSr",
        "outputId": "751591e0-4115-4b5d-90ba-63e7d4f7c486",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.8564]])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "util.pytorch_cos_sim(\n",
        "    get_word_embedding(\"The king is angry\", \"king\"),\n",
        "    get_word_embedding(\"The queen is angry\", \"queen\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtFESD0OGRSr"
      },
      "source": [
        "**例子2.** 在king快乐而queen生气的情况下，\"king\"和\"queen\"嵌入之间的相似度。请注意，它们比之前的例子中相似度较低。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGCyE6i7GRSs",
        "outputId": "f6d6e9eb-970e-488a-fe0b-368498c8f735",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.8273]])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "util.pytorch_cos_sim(\n",
        "    get_word_embedding(\"The king is happy\", \"king\"),\n",
        "    get_word_embedding(\"The queen is angry\", \"queen\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlppsN1WGRSs"
      },
      "source": [
        "**例子3**. 在两个非常不同的上下文中，\"king\"嵌入之间的相似度。即使它们是相同的单词，单词的不同上下文使得嵌入非常不同。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjA9bSeSGRSs",
        "outputId": "6f73bf9d-e7d5-46f6-eb29-05aed06f0db2",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.5740]])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# This is same as before\n",
        "util.pytorch_cos_sim(\n",
        "    get_word_embedding(\"The king and the queen are happy.\", \"king\"),\n",
        "    get_word_embedding(\"The angry and unhappy king\", \"king\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z-CRG4iGRSs"
      },
      "source": [
        "**例子4.** 一个具有两个不同含义的单词的相似度。单词\"bank\"是含糊不清的，它既可以是河岸，也可以是储蓄银行。根据上下文的不同，嵌入也会不同。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZVmxTdhGRSs",
        "outputId": "d661cb69-235a-46a2-a19e-50a78826121f",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.7587]])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "util.pytorch_cos_sim(\n",
        "    get_word_embedding(\"The river bank\", \"bank\"),\n",
        "    get_word_embedding(\"The savings bank\", \"bank\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7j5Jif1GRSs"
      },
      "source": [
        "希望这些例子能给你提供了关于什么是词嵌入的概念。既然我们了解了词嵌入，现在让我们来看看句子嵌入吧！"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODIPpPnQGRSs"
      },
      "source": [
        "### 句子嵌入\n",
        "\n",
        "正如词嵌入是单词的向量表示一样，句子嵌入是句子的向量表示。我们也可以计算段落和文档的嵌入！让我们深入研究一下。\n",
        "\n",
        "我们可以采取三种方法：`[CLS]`池化、最大池化和平均池化。\n",
        "\n",
        "* 平均池化意味着对句子的所有单词嵌入进行平均。\n",
        "* 最大池化意味着取每个维度的单词嵌入的最大值。\n",
        "* `[CLS]`池化意味着使用与`[CLS]`标记相对应的嵌入作为句子嵌入。让我们更深入地研究最后一个，这是最不直观的。\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### [CLS]池化\n",
        "\n",
        "正如我们之前看到的，BERT在句子开头添加了一个特殊标记`[CLS]`。这个标记用于表示整个句子。例如，当有人想要微调BERT模型以执行文本分类时，一个常见的方法是在`[CLS]`嵌入的顶部添加一个线性层。这个想法是`[CLS]`标记将捕捉整个句子的含义。\n",
        "\n",
        "![The hidden state/embedding corresponding to the `CLS` token can be used to fine-tune a classification model.](https://github.com/osanseviero/hackerllama/blob/main/nbs/blog/posts/sentence_embeddings/classification.png?raw=1)\n",
        "\n",
        "我们可以采取相同的方法，使用`[CLS]`标记的嵌入作为句子嵌入。让我们看看在代码中如何实现这一点。我们将使用与之前相同的句子。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIj-B3RiGRSs",
        "outputId": "0ed6306b-38ea-4354-d327-d80c7143a515",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 768])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoded_input = tokenizer(\"This is an example sentence\", return_tensors=\"pt\")\n",
        "model_output = model(**encoded_input)\n",
        "sentence_embedding = model_output[\"last_hidden_state\"][:, 0, :]\n",
        "sentence_embedding.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLGm7PZBGRSs"
      },
      "source": [
        "太棒了！我们获得了模型输出的第一个嵌入，对应于`[CLS]`标记。让我们将这段代码封装到一个函数中。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tX7xDEBbGRSs",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def cls_pooling(model_output):\n",
        "    return model_output[\"last_hidden_state\"][:, 0, :]\n",
        "\n",
        "\n",
        "def get_sentence_embedding(text):\n",
        "    encoded_input = tokenizer(text, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        model_output = model(**encoded_input)\n",
        "    return cls_pooling(model_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9NgyzJFGRSs",
        "outputId": "a2ec7e49-9b1a-40a9-f6c9-1da0e5f8f8d0",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.9261]]) The weather today is beautiful\n",
            "tensor([[0.8903]]) It's raining!\n",
            "tensor([[0.9317]]) Dogs are awesome\n"
          ]
        }
      ],
      "source": [
        "embeddings = [get_sentence_embedding(sentence) for sentence in sentences]\n",
        "query_embedding = get_sentence_embedding(\"Today is a sunny day\")\n",
        "for embedding, sentence in zip(embeddings, sentences):\n",
        "    similarity = util.pytorch_cos_sim(query_embedding, embedding)\n",
        "    print(similarity, sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1XpnmVKGRSt"
      },
      "source": [
        "嗯...这里似乎有些问题🤔 本来期望这个能直接奏效的。\n",
        "\n",
        "事实证明BERT还有一个额外的技巧。如前所述，当BERT被训练时，CLS标记被用来预测两个句子是否连续。为了做到这一点，BERT处理与[CLS]对应的嵌入，并将其通过一个线性层和一个tanh激活函数（参见[此处代码](https://github.com/huggingface/transformers/blob/95754b47a6d4fbdad3440a45762531e8c471c528/src/transformers/models/bert/modeling_bert.py#L652C7-L665)）。这个想法是，线性层和tanh激活函数将学习到`[CLS]`标记的更好表示。这是BERT模型的`pooler`组件，用于获取`model_output.pooler_output`。\n",
        "\n",
        ":::注意\n",
        "\n",
        "这可能听起来有些混乱，所以让我们重复一下这里发生了什么：\n",
        "\n",
        "1. BERT 输出每个标记的嵌入。\n",
        "2. 第一个嵌入对应于`[CLS]`标记。\n",
        "3. `[CLS]`标记经过一个线性层和一个tanh激活函数进行处理，以获取`pooler_output`。\n",
        "\n",
        "在训练过程中，`pooler_output`用于预测两个句子是否连续（BERT的预训练任务之一）。这使得处理`[CLS]`标记比处理原始的`[CLS]`嵌入更有意义。\n",
        "\n",
        ":::"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKTut-HzGRSt"
      },
      "source": [
        "为了表明这里没有什么魔法，我们可以将单词嵌入的列表传递给`model.pooler`，或者简单地从模型输出中获取`pooler_output`。让我们试试看！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fvbktl7FGRSt",
        "outputId": "6aeae18d-d1b8-43aa-c20d-dd541808983e",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-0.9302, -0.4884, -0.4387,  0.8024,  0.3668, -0.3349,  0.9438,  0.3593,\n",
              "        -0.3216, -1.0000], grad_fn=<SliceBackward0>)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.pooler(model_output[\"last_hidden_state\"])[0][:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xm9_v3fFGRSt",
        "outputId": "c8d0d765-6b95-48f2-8bab-b63183d5bca8",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-0.9302, -0.4884, -0.4387,  0.8024,  0.3668, -0.3349,  0.9438,  0.3593,\n",
              "        -0.3216, -1.0000], grad_fn=<SliceBackward0>)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_output[\"pooler_output\"][0][:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6Vsv1KdGRSt"
      },
      "source": [
        "太棒了！正如你所看到的，嵌入的前十个元素是相同的！现在让我们使用这种新的嵌入技术重新计算距离："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEGCAQP5GRSt",
        "outputId": "b80068b1-2e46-48bb-b595-cf87cb48d944",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.9673]], grad_fn=<MmBackward0>) The weather today is beautiful\n",
            "tensor([[0.9029]], grad_fn=<MmBackward0>) It's raining!\n",
            "tensor([[0.8930]], grad_fn=<MmBackward0>) Dogs are awesome\n"
          ]
        }
      ],
      "source": [
        "def cls_pooling(model_output):\n",
        "    return model.pooler(model_output[\"last_hidden_state\"])  # we changed this\n",
        "\n",
        "\n",
        "# This stays the same\n",
        "embeddings = [get_sentence_embedding(sentence) for sentence in sentences]\n",
        "query_embedding = get_sentence_embedding(\"Today is a sunny day\")\n",
        "for embedding, sentence in zip(embeddings, sentences):\n",
        "    similarity = util.pytorch_cos_sim(query_embedding, embedding)\n",
        "    print(similarity, sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RtM2C7eGRSt"
      },
      "source": [
        "好多了！我们刚刚获得了与“今天天气晴朗”最相似的句子。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLyPAUBvGRSt"
      },
      "source": [
        "## Sentence Transformers\n",
        "\n",
        "### 使用transformers库\n",
        "\n",
        "这产生了一些不错的结果，但实际上，这并不比使用Word2Vec或GloVe词嵌入并对它们进行平均要好多少。原因是`[CLS]`标记并不是被训练成一个良好的句子嵌入。它是为了进行下一个句子预测而训练的！\n",
        "介绍 🥁🥁🥁 Sentence Transformers ！ Sentence Transformers（也称为SBERT）具有专门的训练技术，专注于产生高质量的句子嵌入。 就像这篇博客文章的摘要部分一样，让我们使用[all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)模型。 在开始时，我们使用了`sentence-transformers`库，这是一个围绕`transformers`的高级封装库。 让我们先试着走一条艰难的道路吧！ 过程如下：\n",
        "\n",
        "1. 我们对输入句子进行分词。\n",
        "2. 我们通过模型处理这些标记。\n",
        "3. 我们计算标记嵌入的平均值。\n",
        "4. 我们对嵌入进行归一化，以确保嵌入向量具有单位长度。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmvWpuMnGRSt"
      },
      "source": [
        "就像之前一样，我们可以加载模型和分词器，对句子进行分词，然后将其传递给模型。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyeAe-PgGRSt",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "encoded_input = tokenizer(\"Today is a sunny day\", return_tensors=\"pt\")\n",
        "model_output = model(**encoded_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnXsHHxDGRSu"
      },
      "source": [
        "到目前为止，我们所做的与之前非常相似，只是我们使用了不同的模型。接下来的步骤是进行池化。虽然以前我们使用了[CLS]池化，但句子转换器通常使用平均池化或最大池化。让我们试试看！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkJxqrhxGRSu",
        "outputId": "26ac5ea9-802d-45fc-9a7b-e67493dfee87",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 7, 384])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "token_embeddings = model_output[\"last_hidden_state\"]\n",
        "token_embeddings.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHIBuX8MGRSu"
      },
      "source": [
        "请注意，使用此模型，每个嵌入都更小（384个值而不是768个）。现在我们可以计算嵌入的平均值以获得句子嵌入。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxSe5xGUGRSu",
        "outputId": "b29a9daf-7f48-4820-dd25-21013d2e7291",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 384])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mean_embedding = torch.mean(token_embeddings, dim=1)\n",
        "mean_embedding.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaEqD1yMGRSu"
      },
      "source": [
        "最后一步是进行归一化。归一化确保嵌入向量具有单位长度，这意味着它的长度（或大小）为1。\n",
        "\n",
        ":::{.callout-note title=\"什么是归一化\"}\n",
        "\n",
        "要理解为什么我们进行归一化，重新审视一些向量数学是有帮助的。 对于一个具有分量（v1，v2，…，vn）的向量 v，它的长度被定义为\n",
        "\n",
        "$$\n",
        "\\| \\mathbf{v} \\| = \\sqrt{v_1^2 + v_2^2 + \\ldots + v_n^2}\n",
        "$$\n",
        "\n",
        "当归一化一个向量时，我们缩放值，使得向量长度为1。 这是通过将每个向量元素除以向量的大小来完成的。\n",
        "\n",
        "$$\n",
        "\\mathbf{u} = \\frac{\\mathbf{v}}{\\| \\mathbf{v} \\|}\n",
        "$$\n",
        "\n",
        ":::\n",
        "\n",
        "这在我们想要比较向量时特别有用。例如，如果我们想要计算两个向量之间的余弦相似度，我们通常比较它们的方向而不是它们的长度。将向量归一化可以确保每个向量对相似度的计算有均等的贡献。我们稍后会更多讨论嵌入（embedding）比较的内容！让我们试试看！\n",
        "\n",
        "\n",
        ":::{.callout-note}\n",
        "\n",
        "实际上，我们正在使用余弦相似度来计算嵌入向量之间的相似性。正如我们稍后在博客文章中将会看到的，计算余弦相似度时嵌入向量的长度并不重要，但如果我们想要尝试其他测量距离的方法，对它们进行归一化仍然是一个好主意。\n",
        "\n",
        "\n",
        ":::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwPoanTxGRSu",
        "outputId": "8cdc0a48-5114-435e-ae7e-d8aa0b795075",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 384])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "normalized_embedding = F.normalize(mean_embedding)\n",
        "normalized_embedding.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq3E_UylGRSu"
      },
      "source": [
        "让我们将这个功能封装成一个函数！\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdTMo03eGRSu",
        "outputId": "712a6476-816e-4c5a-ac48-c1982f5ef174",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-0.0926,  0.5913,  0.5535,  0.4214,  0.2129])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def mean_pooling(model_output):\n",
        "    return torch.mean(model_output[\"last_hidden_state\"], dim=1)\n",
        "\n",
        "\n",
        "def get_sentence_embedding(text):\n",
        "    encoded_input = tokenizer(text, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        model_output = model(**encoded_input)\n",
        "    sentence_embeddings = mean_pooling(model_output)\n",
        "    return F.normalize(sentence_embeddings)\n",
        "\n",
        "\n",
        "get_sentence_embedding(\"Today is a sunny day\")[0][:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJb14CHsGRSu"
      },
      "source": [
        "在实践中，您可能需要编码一批句子，所以我们需要做一些改动\n",
        "\n",
        "* 修改分词操作，以便我们应用“截断”（如果句子长度超过最大长度则进行裁剪）和“填充”（在句子末尾添加[PAD]标记）。\n",
        "\n",
        "* 修改池化操作，以便我们考虑注意力掩码。注意力掩码是一个由0和1组成的向量，它指示哪些令牌是真实的，哪些是填充的。在计算平均值时，我们希望忽略填充令牌！\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vbO5EzhGRSu",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[\"last_hidden_state\"]\n",
        "    input_mask_expanded = (\n",
        "        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    )\n",
        "    return torch.sum(token_embeddings, 1) / torch.clamp(\n",
        "        input_mask_expanded.sum(1), min=1e-9\n",
        "    )\n",
        "\n",
        "\n",
        "# This now receives a list of sentences\n",
        "def get_sentence_embedding(sentences):\n",
        "    encoded_input = tokenizer(\n",
        "        sentences, padding=True, truncation=True, return_tensors=\"pt\"\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        model_output = model(**encoded_input)\n",
        "    sentence_embeddings = mean_pooling(model_output, encoded_input[\"attention_mask\"])\n",
        "    return F.normalize(sentence_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W10LqslDGRSu",
        "outputId": "0fce1782-9653-4808-ade3-3adef0a0b538",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-0.0163,  0.1041,  0.0974,  0.0742,  0.0375])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query_embedding = get_sentence_embedding(\"Today is a sunny day\")[0]\n",
        "query_embedding[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlwQivO9GRSv"
      },
      "source": [
        "我们得到了相同的结果，太好了！现在让我们重复之前的搜索示例。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwVLiypuGRSv",
        "outputId": "0c8ee79f-e337-41e0-981a-57d979973f44",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.7344]]) The weather today is beautiful\n",
            "tensor([[0.4180]]) It's raining!\n",
            "tensor([[0.1060]]) Dogs are awesome\n"
          ]
        }
      ],
      "source": [
        "embeddings = [get_sentence_embedding(sentence) for sentence in sentences]\n",
        "for embedding, sentence in zip(embeddings, sentences):\n",
        "    similarity = util.pytorch_cos_sim(query_embedding, embedding)\n",
        "    print(similarity, sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_68Q_PuGRSv"
      },
      "source": [
        "很好！与普通的BERT [CLS]池化嵌入相比，句子转换器嵌入更有意义，并且在无关向量之间的差异更大！\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QA5PIJlGRSv"
      },
      "source": [
        ":::callout\n",
        "\n",
        "何时使用每种池化策略？这取决于任务。\n",
        "\n",
        "* `[CLS]` 当transformer模型已经在特定的下游任务上进行了微调，使得`[CLS]`标记非常有用时，通常会使用`[CLS]`池化。\n",
        "\n",
        "* 平均池化通常在没有在下游任务上微调的模型上更有效。它确保句子的所有部分在嵌入中都有平等的代表性，并且可以用于需要捕获所有标记影响的长句子上。\n",
        "\n",
        "* 最大池化可以用来捕获句子中最重要特征。如果特定的关键词非常有信息量，这会非常有用，但它可能会忽略更微妙的内容。\n",
        "\n",
        "\n",
        "在实践中，池化方法将与模型一起存储，您不必担心它。如果没有指定方法，平均池化通常是一个很好的默认选择。\n",
        "\n",
        ":::"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys59YQUwGRSv"
      },
      "source": [
        "### 使用 sentence-transformers 库\n",
        "\n",
        "这相对容易，但是`sentence-transformers`库使我们能够更轻松地完成所有这些工作！以下是与摘要部分相同的代码。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqJvkEDNGRSv",
        "outputId": "66aba6ec-5c2a-4684-a6c0-1ba6732bd3f1",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.7344]]) The weather today is beautiful\n",
            "tensor([[0.4180]]) It's raining!\n",
            "tensor([[0.1060]]) Dogs are awesome\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# We load the model\n",
        "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "query_embedding = model.encode(\"Today is a sunny day\")\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "for embedding, sentence in zip(embeddings, sentences):\n",
        "    similarity = util.pytorch_cos_sim(query_embedding, embedding)\n",
        "    print(similarity, sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "882yWtx-GRSv"
      },
      "source": [
        "这相当强大！如果您不使用机器学习来实现识别重复问题的功能，您可能需要实现一个词汇搜索系统（查找输入问题的精确匹配）、一个模糊搜索系统（查找输入问题的近似匹配）或一个统计搜索系统（查找输入问题中单词的频率）。\n",
        "\n",
        "有了嵌入，我们可以轻松地找到相似的问题，而不需要实现任何这些系统，并且取得了出色的结果！"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8enk5-aGRSw"
      },
      "source": [
        "以下图片是如何使用嵌入来找到能够回答用户问题的代码的一个很好的例子\n",
        "\n",
        "![Image of code search](https://github.com/osanseviero/hackerllama/blob/main/nbs/blog/posts/sentence_embeddings/search.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf_DF8h2GRSw"
      },
      "source": [
        "### 嵌入维度\n",
        "\n",
        "正如您之前所见，我们使用的模型all-MiniLM-L6-v2生成了384个值的句子嵌入。这是模型的一个超参数，可以进行更改。嵌入大小越大，嵌入能够捕获的信息就越多。然而，较大的嵌入计算和存储成本更高。\n",
        "\n",
        "流行的开源模型的嵌入维度从384到1024不等。截至撰写本文时，最好的当前模型的嵌入维度为4096个值，但与其他模型相比，该模型要大得多（70亿个参数）。在封闭源世界中，Cohere具有从384到4096维的API，OpenAI的嵌入维度为1536等等。**嵌入维度是一种权衡**。如果您使用非常大的嵌入，您可能会获得更好的结果，但您也将不得不为托管和推理付出更多的费用。如果您使用向量数据库，您还将不得不为存储支付更多费用。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JeEP-MzGRSw"
      },
      "source": [
        "### 序列长度\n",
        "\n",
        "Transformer模型的一个限制是它们具有最大的序列长度。这意味着它们只能处理一定数量的标记。例如，BERT的最大上下文长度为512个标记。这意味着如果您想要编码一个超过512个标记的句子，您将不得不找到解决此限制的方法。例如，您可以将句子拆分为多个包含512个标记的句子，然后对嵌入进行平均。这不是理想的，因为模型将无法捕获整个句子的上下文。\n",
        "\n",
        "这对于大多数用例来说不是问题，但对于长文档可能会有问题。例如，如果您想要编码一个1000字的文档，您将不得不将其拆分为多个包含512个标记的句子。这不是理想的，因为模型将无法捕获整个文档的上下文。另一种方法是首先生成文本摘要，然后对摘要进行编码。如果您想编码长文档，这是一个好方法，但会需要一个可能太慢的好的摘要模型。或者，您可能知道文档的特定部分是否合适（例如摘要、引言、结论等），如果对于您的任务来说这部分是最有意义的，则只对该部分进行编码。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VX8t4XAsGRSw"
      },
      "source": [
        "## 应用一： 查找最相似的Quora相似问题"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8osGLZ9GRSx"
      },
      "source": [
        "我们将使用开源的[Quora数据集](https://huggingface.co/datasets/quora)，其中包含来自Quora的40万对问题。我们暂时不会训练模型，而是仅使用嵌入来查找给定新问题的相似问题。让我们开始吧！\n",
        "\n",
        "我们的第一步是加载数据 - 为此，我们将使用 `datasets` 库。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dn9wYBn1GRSx",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9OKJYweGRSx",
        "outputId": "4ffad3c2-72b1-4335-f147-7e36b29d0de4",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['questions', 'is_duplicate'],\n",
              "    num_rows: 404290\n",
              "})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"quora\")[\"train\"]\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpSR40ohGRSx"
      },
      "source": [
        "为了快速查看 `Dataset` 对象中的数据，我们可以将其转换为 Pandas 的 `DataFrame` 并查看前几行。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeVa7mS4GRSx",
        "outputId": "6048ce1a-409d-4e0e-fbd7-1609246bece2",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>questions</th>\n",
              "      <th>is_duplicate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>{'id': [1, 2], 'text': ['What is the step by s...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>{'id': [3, 4], 'text': ['What is the story of ...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>{'id': [5, 6], 'text': ['How can I increase th...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>{'id': [7, 8], 'text': ['Why am I mentally ver...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>{'id': [9, 10], 'text': ['Which one dissolve i...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           questions  is_duplicate\n",
              "0  {'id': [1, 2], 'text': ['What is the step by s...         False\n",
              "1  {'id': [3, 4], 'text': ['What is the story of ...         False\n",
              "2  {'id': [5, 6], 'text': ['How can I increase th...         False\n",
              "3  {'id': [7, 8], 'text': ['Why am I mentally ver...         False\n",
              "4  {'id': [9, 10], 'text': ['Which one dissolve i...         False"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.to_pandas().head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f1_H6pWGRSx"
      },
      "source": [
        "好的，所以每个样本都是一个字典。我们在这里不关心 `is_duplicate` 列。我们的目标是找出数据集中是否有任何问题与新问题相似。让我们处理数据集，使我们只有一个问题列表。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_1OSumrGRSx",
        "outputId": "1a385960-9a88-40f0-9cdf-f489823f864b",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "537362"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus_questions = []\n",
        "for d in dataset:\n",
        "    corpus_questions.append(d[\"questions\"][\"text\"][0])\n",
        "    corpus_questions.append(d[\"questions\"][\"text\"][1])\n",
        "corpus_questions = list(set(corpus_questions))  # Remove duplicates\n",
        "len(corpus_questions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBCgrscQGRSx"
      },
      "source": [
        "一步是嵌入所有的问题。我们将使用 `sentence-transformers` 库来实现这一步。我们将使用 [`quora-distilbert-multilingual` 模型](https://huggingface.co/sentence-transformers/quora-distilbert-multilingual)，这是一个为100种语言训练的模型，专门用于Quora风格的问题。这是一个较大的模型，因此稍微慢一些。它还会生成更大的嵌入，有768个值。\n",
        "\n",
        "为了快速获得一些结果，而不必等待模型处理所有问题需要五分钟的时间，我们只处理前100000个问题。在实践中，当进行实验时，您可以处理所有问题或对问题进行洗牌，并处理其中的一个随机子集。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "30cc6aa1ede9402ea9cd36fb9aed9376"
          ]
        },
        "id": "bG9XbqNxGRSx",
        "outputId": "73249c91-000e-4dfe-c6cb-984084ddc6d2",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30cc6aa1ede9402ea9cd36fb9aed9376",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/3125 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = SentenceTransformer(\"quora-distilbert-multilingual\")\n",
        "questions_to_embed = 100000\n",
        "corpus_embeddings = model.encode(\n",
        "    corpus_questions[:questions_to_embed],\n",
        "    show_progress_bar=True,\n",
        "    convert_to_tensor=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWiVGWc1GRSx",
        "outputId": "34396ccc-c4d8-4d09-a8d1-ed7f4e89c547",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([100000, 768])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus_embeddings.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-paWR_m2GRSy"
      },
      "source": [
        "我们在20秒内仅获得了100,000个嵌入，即使这个Sentence Transformer模型并不小，我在我的显卡性能较弱的计算机上运行。与生成模型不同，它们是自回归的并且通常速度要慢得多，基于BERT的模型非常快！"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-iNlnPIGRSy"
      },
      "source": [
        "现在让我们编写一个函数，来搜索语料库中最相似的问题。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IZQ_y8TGRSy",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "\n",
        "def search(query):\n",
        "    start_time = time.time()\n",
        "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "    results = util.semantic_search(query_embedding, corpus_embeddings)\n",
        "    end_time = time.time()\n",
        "\n",
        "    print(\"Results (after {:.3f} seconds):\".format(end_time - start_time))\n",
        "    # We look at top 5 results\n",
        "    for result in results[0][:5]:\n",
        "        print(\n",
        "            \"{:.3f}\\t{}\".format(result[\"score\"], corpus_questions[result[\"corpus_id\"]])\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73gxcNb6GRSy",
        "outputId": "5237118d-c609-44ba-ba16-38fb79980b95",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results (after 0.612 seconds):\n",
            "0.982\tWhat is the best online resource to learn Python?\n",
            "0.980\tWhere I should learn Python?\n",
            "0.980\tWhat's the best way to learn Python?\n",
            "0.980\tHow do I learn Python in easy way?\n",
            "0.979\tHow do I learn Python systematically?\n"
          ]
        }
      ],
      "source": [
        "search(\"How can I learn Python online?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q57WQ61tGRSy"
      },
      "source": [
        "让我们尝试用西班牙语！"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hH5V4WIrGRSy",
        "outputId": "b2c1663d-442a-4379-bd79-8583d21a94e0",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results (after 0.016 seconds):\n",
            "0.980\tWhat are the best websites to learn Python?\n",
            "0.980\tHow can I start learning the developing of websites using Python?\n",
            "0.979\tHow do I learn Python in easy way?\n",
            "0.976\tHow can I learn Python faster and effectively?\n",
            "0.976\tHow can I learn advanced Python?\n"
          ]
        }
      ],
      "source": [
        "search(\"Como puedo aprender Python online?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvQhLMeiGRSy"
      },
      "source": [
        "看起来效果相当不错！请注意，虽然我们的模型可以处理其他语言的查询，比如上面的示例中的西班牙语，但嵌入是针对英文问题生成的。这意味着模型将无法在其他语言中找到相似的问题。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuX3jBsHGRSy"
      },
      "source": [
        "## 嵌入之间的距离\n",
        "\n",
        "### 余弦相似度\n",
        "\n",
        "到目前为止，我们一直在计算嵌入之间的余弦相似度。这是一个介于0和1之间的数字，表示两个嵌入有多相似。值为1意味着嵌入是相同的，而值为0则意味着嵌入完全不同。到目前为止，我们把它当作一个黑匣子来使用，所以让我们更深入地了解一下。\n",
        "\n",
        "余弦相似度允许我们比较两个向量的相似程度，而不考虑它们的大小。例如，如果我们有两个向量，[1, 2, 3] 和 [2, 4, 6]，它们在方向上非常相似，但它们的大小是不同的。余弦相似度将接近于1，表明它们非常相似。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8z4MtJeVGRSy",
        "outputId": "85c586ac-390c-4b85-cfd0-71ac432039e4",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.9926]])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = torch.FloatTensor([1, 2, 3])\n",
        "b = torch.FloatTensor([2, 3, 4])\n",
        "util.cos_sim(a, b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fXeje2SGRSy"
      },
      "source": [
        "让我们绘制这两个向量。正如你所看到的，它们在方向上非常相似，但它们的大小是不同的。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdFAAgoWGRSy",
        "outputId": "c7765f53-cab5-4867-865f-082bca699afe",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1., 2., 3.])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEA9cBlxGRSz",
        "outputId": "4d1c5d08-39f8-4cc4-d6a4-52b51202d792",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "V = np.array([a.tolist(), b.tolist()])\n",
        "origin = np.array([[0, 0], [0, 0]])  # origin point\n",
        "\n",
        "plt.quiver(*origin, V[:, 0], V[:, 1], color=[\"r\", \"b\", \"g\"], scale=10)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3saSuq4GRSz"
      },
      "source": [
        "让我们深入了解其数学原理。余弦相似度被定义为向量的点积除以它们的大小的乘积：\n",
        "\n",
        "$$\n",
        "\\text{cosine similarity}(\\mathbf{A}, \\mathbf{B}) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\|\\mathbf{B}\\|}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_ElletGGRSz"
      },
      "source": [
        "我们已经在博客文章开头讨论了大小，我们需要计算一个向量分量的平方和的平方根。\n",
        "\n",
        "$$\n",
        "\\|\\mathbf{A}\\| = \\sqrt{1^2 + 2^2 + 3^2} = \\sqrt{14}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\|\\mathbf{B}\\| = \\sqrt{2^2 + 3^2 + 4^2} = \\sqrt{29}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRPsDW3pGRSz"
      },
      "source": [
        "我们还需要计算向量的点积，点积被定义为对应向量分量的乘积之和。\n",
        "\n",
        "$$\n",
        "\\mathbf{A} \\cdot \\mathbf{B} = \\sum_{i=1}^{n} A_i B_i\n",
        "$$\n",
        "\n",
        "在这种情况下，A和B的点积如下所示：\n",
        "\n",
        "$$\n",
        "\\mathbf{A} \\cdot \\mathbf{B} = 1 \\times 2 + 2 \\times 3 + 3 \\times 4 = 2 + 6 + 12 = 20\n",
        "$$\n",
        "\n",
        "最后，我们可以通过进行以下计算来计算余弦相似度:\n",
        "\n",
        "$$\n",
        "\\text{cosine similarity}(\\mathbf{A}, \\mathbf{B}) = \\frac{20}{\\sqrt{14} \\sqrt{29}} = 0.992583\n",
        "$$\n",
        "\n",
        "与我们上面的结果相匹配。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmWHmJO_GRSz"
      },
      "source": [
        ":::{.温馨提示}\n",
        "\n",
        "你能想到两个余弦相似度为1的向量吗？想想方向相同但大小不同的向量。\n",
        "\n",
        ":::"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihhqzJqrGRSz"
      },
      "source": [
        "### 点积\n",
        "\n",
        "余弦相似度不考虑大小，但在某些情况下，大小可能是有意义的。在这些情况下，**点积**是一个更好的度量标准。这意味着长度或更详细的具有相似内容的句子可能会比长度较短的具有相似内容的句子具有更高的相似度得分，这是由于它们的大小。\n",
        "\n",
        "点积被定义为对应向量分量的乘积之和（这是我们之前做的！）\n",
        "\n",
        "$$\n",
        "\\mathbf{A} \\cdot \\mathbf{B} = \\sum_{i=1}^{n} A_i B_i\n",
        "$$\n",
        "\n",
        "如果你看一下余弦相似度公式，如果假设向量已经被归一化（即它们的大小为1），那么余弦相似度等同于点积。这意味着余弦相似度是一个归一化的点积。\n",
        "\n",
        "让我们创建一个新的向量，[4, 6, 8]。这个向量与[2, 3, 4]的方向相同，但是长度是它的两倍。让我们计算[1, 2, 3]与[2, 3, 4]和[4, 6, 8]的点积。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcrViGDIGRSz",
        "outputId": "6983ef21-e069-42cd-f931-8146b1d38d18",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine Similarity between a and b: tensor([[0.9926]])\n",
            "Cosine Similarity between a and c: tensor([[0.9926]])\n",
            "Dot product between a and b: 20.0\n",
            "Dot product between a and c: 40.0\n"
          ]
        }
      ],
      "source": [
        "c = torch.FloatTensor([4, 6, 8])\n",
        "\n",
        "print(f\"Cosine Similarity between a and b: {util.cos_sim(a, b)}\")\n",
        "print(f\"Cosine Similarity between a and c: {util.cos_sim(a, c)}\")\n",
        "\n",
        "print(f\"Dot product between a and b: {torch.dot(a, b)}\")\n",
        "print(f\"Dot product between a and c: {torch.dot(a, c)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3hV0oaKGRSz"
      },
      "source": [
        "这是有道理的！因为b和c具有相同的角度，所以a和b以及a和c之间的余弦相似度是相同的。然而，由于c比b更长，所以a和c之间的点积更高。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skP0FZNGGRSz",
        "outputId": "ba4e3dd9-5fcd-4cd7-fe7d-bb5121eabad2",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "V = np.array([a.tolist(), b.tolist(), c.tolist()])\n",
        "origin = np.array([[0, 0, 0], [0, 0, 0]])  # origin point\n",
        "\n",
        "plt.quiver(*origin, V[:, 0], V[:, 1], color=[\"r\", \"b\", \"g\"], scale=20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8dcTWR-GRSz"
      },
      "source": [
        "### 欧几里得距离\n",
        "\n",
        "欧几里得距离是通过测量两个向量之间的直线距离来计算的距离。与点积一样，欧几里得距离考虑了向量的大小。我不会深入解释这两个度量标准，但主要思想是，点积度量了一个向量延伸到另一个向量方向的程度，而欧几里得距离则度量了两个向量之间的直线距离。它被定义为各个向量分量之间差的平方和的平方根。其定义为：\n",
        "\n",
        "$$\n",
        "\\text{Euclidean Distance}(\\mathbf{A}, \\mathbf{B}) = \\sqrt{\\sum_{i=1}^{n} (A_i - B_i)^2}\n",
        "$$\n",
        "\n",
        "In practice, you can use the Squared Euclidean (L2-Squared)\n",
        "\n",
        "$$\n",
        "\\text{Squared Euclidean}(\\mathbf{A}, \\mathbf{B}) = \\sum_{i=1}^{n} (A_i - B_i)^2\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb_dPIOeGRSz"
      },
      "source": [
        "### 选择一个评分函数\n",
        "\n",
        "我们刚刚学习了点积、余弦相似度和欧几里得距离。何时使用哪一个？\n",
        "\n",
        "这取决于模型！某些模型将以产生归一化嵌入的方式进行训练。在这种情况下，点积、余弦相似度和欧几里得距离都会产生相同的结果。\n",
        "\n",
        "其他模型不是以产生归一化嵌入的方式进行训练 - 它们被调整为进行点积。在这种情况下，点积将是在向量空间中找到最接近项的最佳函数。即使如此，如果大小不重要，我们仍然可以像在前面的部分中那样进行归一化。您可以根据您的用例使用不同的距离函数。具有归一化嵌入的模型将更喜欢较短的句子，而具有非归一化嵌入的模型将更喜欢较长的句子。这是因为较长句子的嵌入的大小会更大。\n",
        "\n",
        "| Distance function | Values | When to use |\n",
        "| ----------------- | ------ | ----------- |\n",
        "| Cosine similarity | [-1, 1] | When the magnitude is not important |\n",
        "| Dot product       | [-inf, inf] | When the magnitude is important |\n",
        "| Euclidean distance | [0, inf] | When the magnitude is important |\n",
        "\n",
        "简要总结一下：\n",
        "\n",
        "* **余弦相似度** 关注向量之间的夹角。它是一个归一化的点积。\n",
        "* **点积** 关注向量的大小和角度。\n",
        "* **欧几里得距离** 用于测量向量之间的空间距离。\n",
        "\n",
        "还有其他距离函数，例如曼哈顿距离，但这些是常见的并且对我们的用例很有用！"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4E_ETVQcGRS0"
      },
      "source": [
        "## 扩展\n",
        "\n",
        "到目前为止，我们只处理了几个句子。在实践中，您可能需要处理数百万个嵌入，而我们并不总是能够计算到所有嵌入的距离（这称为蛮力搜索）。\n",
        "\n",
        "一种方法是使用近似最近邻算法。这些算法将数据划分为相似嵌入的桶。这使我们能够快速找到最接近的嵌入，而不必计算到所有嵌入的距离。这并不是精确的，因为一些相似度很高的向量可能仍然被错过。您可以使用不同的库来实现这一点，比如Spotify的[Annoy](https://github.com/spotify/annoy)和Facebook的[Faiss](https://github.com/facebookresearch/faiss)。像Pinecone和Weaviate这样的向量数据库也使用最近邻技术，以便在毫秒内搜索数百万个对象。\n",
        "\n",
        "现在，让我们来看一个有趣的应用，其中扩展性问题变得更加明显。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGtDwaW3GRS0"
      },
      "source": [
        "### 应用二：释义挖掘\n",
        "\n",
        "到目前为止，通过语义搜索，我们一直在寻找与查询句子最相似的句子。在**释义挖掘**中，目标是在一个非常大的语料库中找到意思相似的文本。让我们拿我们的Quora数据集来看看是否能找到相似的问题。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opPESt9IGRS0",
        "outputId": "80065b2d-f517-4ea4-8b42-1017c83da287",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['',\n",
              " 'What are the Nostradamus Predictions for the 2017?',\n",
              " 'Is it expensive to take music lessons?',\n",
              " 'what are the differences between first world and third world countries? Are there any second world countries?',\n",
              " 'How much is a 1963 2 dollar bill with a red seal worth?',\n",
              " 'What is the capital of Finland?',\n",
              " 'Which is the best project management app for accounting companies?',\n",
              " \"What is Dire Straits' best album ever?\",\n",
              " 'How does Weapon Silencers work?',\n",
              " 'How should we study in medical school?']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "questions_to_embed = 10\n",
        "short_corpus_questions = corpus_questions[:questions_to_embed]\n",
        "short_corpus_questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekGYNnMzGRS0",
        "outputId": "5de36cf3-f884-4d09-fd32-14f5be3123be",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results (after 0.000 seconds):\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.7863, 0.6348, 0.7524, 0.7128, 0.7620, 0.6928, 0.7316, 0.6973,\n",
              "         0.6602],\n",
              "        [0.7863, 1.0000, 0.7001, 0.8369, 0.8229, 0.8093, 0.7694, 0.8111, 0.7849,\n",
              "         0.7157],\n",
              "        [0.6348, 0.7001, 1.0000, 0.6682, 0.7346, 0.7228, 0.7257, 0.7434, 0.7529,\n",
              "         0.7616],\n",
              "        [0.7524, 0.8369, 0.6682, 1.0000, 0.7484, 0.8042, 0.6713, 0.7560, 0.7336,\n",
              "         0.6901],\n",
              "        [0.7128, 0.8229, 0.7346, 0.7484, 1.0000, 0.7222, 0.7419, 0.7603, 0.8080,\n",
              "         0.7145],\n",
              "        [0.7620, 0.8093, 0.7228, 0.8042, 0.7222, 1.0000, 0.7327, 0.7542, 0.7349,\n",
              "         0.6992],\n",
              "        [0.6928, 0.7694, 0.7257, 0.6713, 0.7419, 0.7327, 1.0000, 0.7820, 0.7270,\n",
              "         0.7513],\n",
              "        [0.7316, 0.8111, 0.7434, 0.7560, 0.7603, 0.7542, 0.7820, 1.0000, 0.7432,\n",
              "         0.7151],\n",
              "        [0.6973, 0.7849, 0.7529, 0.7336, 0.8080, 0.7349, 0.7270, 0.7432, 1.0000,\n",
              "         0.7243],\n",
              "        [0.6602, 0.7157, 0.7616, 0.6901, 0.7145, 0.6992, 0.7513, 0.7151, 0.7243,\n",
              "         1.0000]], device='cuda:0')"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = SentenceTransformer(\"quora-distilbert-multilingual\")\n",
        "embeddings = model.encode(short_corpus_questions, convert_to_tensor=True)\n",
        "\n",
        "# Compute distance btween all embeddings\n",
        "start_time = time.time()\n",
        "distances = util.pytorch_cos_sim(embeddings, embeddings)\n",
        "end_time = time.time()\n",
        "\n",
        "print(\"Results (after {:.3f} seconds):\".format(end_time - start_time))\n",
        "distances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ku4yhVBeGRS0"
      },
      "source": [
        "太棒了！我们刚刚计算了10个嵌入与10个嵌入之间的距离。速度非常快。现在让我们尝试使用1000个查询。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "90f929bf62b04c5180c9573320320d4e"
          ]
        },
        "id": "YgbVY6SsGRS0",
        "outputId": "ace8b807-9f70-4a73-b29a-51536374c1b7",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90f929bf62b04c5180c9573320320d4e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/625 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results (after 0.000 seconds):\n"
          ]
        }
      ],
      "source": [
        "def compute_embeddings_slow(questions, n=10):\n",
        "    embeddings = model.encode(\n",
        "        questions[:n], show_progress_bar=True, convert_to_tensor=True\n",
        "    )\n",
        "\n",
        "    # Compute distance btween all embeddings\n",
        "    start_time = time.time()\n",
        "    distances = util.pytorch_cos_sim(embeddings, embeddings)\n",
        "    end_time = time.time()\n",
        "\n",
        "    return distances, end_time - start_time\n",
        "\n",
        "\n",
        "_, s = compute_embeddings_slow(corpus_questions, 20000)\n",
        "print(\"Results (after {:.3f} seconds):\".format(s))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ij1Q4tOsGRS0"
      },
      "source": [
        "好的，这仍然很快！让我们看看一些其他的值。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "bc00ef68a4dd45ec822cb736f62f0c6d",
            "7ca7ff71b91f48aa9c5776803dabc406",
            "e6de0384bb7e4c5eb497cc1d847f364d",
            "1bcf39b41a744a2da167afeab7359ae7"
          ]
        },
        "id": "lsIoyEBNGRS0",
        "outputId": "2ac7488c-0f35-4495-e1b7-fc8e851e5398",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc00ef68a4dd45ec822cb736f62f0c6d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ca7ff71b91f48aa9c5776803dabc406",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/313 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e6de0384bb7e4c5eb497cc1d847f364d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/626 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1bcf39b41a744a2da167afeab7359ae7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/938 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Time (seconds)')"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n_queries = [1, 10001, 20001, 30001]  # If I keep going my computer explodes\n",
        "times = []\n",
        "\n",
        "for n in n_queries:\n",
        "    _, s = compute_embeddings_slow(corpus_questions, n)\n",
        "    times.append(s)\n",
        "    torch.cuda.empty_cache()  # Clear GPU cache\n",
        "\n",
        "plt.plot(n_queries, times)\n",
        "plt.xlabel(\"Number of queries\")\n",
        "plt.ylabel(\"Time (seconds)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WzxjtEwGRS0"
      },
      "source": [
        "上述算法具有二次运行时间，因此如果我们不断增加查询数量，它将无法很好地扩展。对于更大的集合，我们可以使用[释义挖掘技术](https://www.sbert.net/examples/applications/paraphrase-mining/README.html)，这种方法更复杂、更高效。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "207c7cf4d03543b9ab3f5edc276cb7fd"
          ]
        },
        "id": "Q8FuOoOVGRS0",
        "outputId": "610f1d69-c8b5-4828-99e3-7d3612235ac0",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "207c7cf4d03543b9ab3f5edc276cb7fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/3125 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "paraphrases = util.paraphrase_mining(\n",
        "    model, corpus_questions[:100000], show_progress_bar=True\n",
        ")\n",
        "end_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdQYrzwRGRS0",
        "outputId": "bc49ea46-1f57-424d-9456-7ba1095e418d",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "250976"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(paraphrases)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxk2wxYEGRS1",
        "outputId": "e6d94da7-737a-4cdf-89b1-051ba7eca5db",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[0.999999463558197, 18862, 24292],\n",
              " [0.9999779462814331, 10915, 61354],\n",
              " [0.9999630451202393, 60527, 86890]]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "paraphrases[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Rx1cK8kGRS1"
      },
      "source": [
        "第一个值是分数，第二个是语料库问题的索引，第三个是另一个语料库问题的索引。分数表示两个问题有多相似。\n",
        "\n",
        "非常好！我们只需要\n",
        "1. 计算了10万个问题的嵌入。\n",
        "2. 获取了最相似的句子，并\n",
        "3. 对它们进行了排序。\n",
        "\n",
        "所有这些在20秒内完成！让我们看看具有最高相似度的5个匹配项。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5NnP-YvGRS1",
        "outputId": "9e5d05f8-3772-43d0-fb2a-1b8a5787f47e",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.000\tHow do I  increase traffic on my site? and How do I increase traffic on my site?\n",
            "1.000\twho is the best rapper of all time? and Who is the best rapper of all time?\n",
            "1.000\tHow can I become an automobile engineer? and How can I become a automobile engineer?\n",
            "1.000\tI made a plasma vortex at my home, but why doesn't it produce a zapping sound like at time when we see sparks and does the air nearby it ionizes? and I made a plasma vortex at my home, but why doesn't it produce a zapping sound like at time when we see sparks and does the air nearby it, ionizes?\n",
            "1.000\tWhy was Cyrus Mistry removed as the chairman of Tata Sons? and Why was Cyrus Mistry removed as the Chairman of Tata Sons?\n"
          ]
        }
      ],
      "source": [
        "for score, i, j in paraphrases[:5]:\n",
        "    print(\"{:.3f}\\t{} and {}\".format(score, corpus_questions[i], corpus_questions[j]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEP21atuGRS1"
      },
      "source": [
        "这种方法是如何工作的？语料库被划分为较小的块，这使我们能够管理内存和计算资源的使用。块划分有两种方式：\n",
        "\n",
        "* **查询块大小：** 决定了考虑作为潜在释义的句子的数量。这是与查询句子进行比较的句子数量，并由 `query_chunk_size` 控制（默认为5000）。\n",
        "* **语料库块大小：** 决定了同时进行比较的语料库块的数量。这由 `corpus_chunk_size` 控制（默认为100000）。\n",
        "\n",
        "例如，默认参数下，该算法每次处理5000个句子，将这些句子与语料库中剩余部分的10万个句子的块进行比较。该算法专注于获取**前几个匹配项** - 使用 `top_k`，对于查询块中的每个句子，算法只选择语料库块中的前 k 个匹配项。这意味着该算法不会找到所有的匹配项，但它会找到前几个匹配项。这是一个很好的折衷，因为我们通常不需要所有的匹配项，只需要前几个匹配项。\n",
        "\n",
        "这两个参数使得过程更有效，因为处理数据的较小子集在计算上更容易。它还有助于节省内存，因为我们不必将整个语料库加载到内存中来计算相似度。找到这些参数的合适值是速度和准确度之间的折衷。值越大，结果越准确，但算法越慢。\n",
        "\n",
        ":::{.温馨提示}\n",
        "\n",
        "您可以使用 `max_pairs` 来限制返回的配对数。\n",
        "\n",
        ":::\n",
        "\n",
        "这是算法的伪代码：\n",
        "\n",
        "```python\n",
        "# Initialize an empty list to store the results\n",
        "results = []\n",
        "\n",
        "for query_chunk in query_chunks:\n",
        "    for corpus_chunk in corpus_chunks:\n",
        "        # Compute the similarity between the query chunk and the corpus chunk\n",
        "        similarity = compute_similarity(query_chunk, corpus_chunk)\n",
        "        # Get the top k matches in the other chunk\n",
        "        top_k_matches = similarity.top_k(top_k)\n",
        "        # Add the top k matches to the results\n",
        "        results.add(top_k_matches)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8ztHnh-GRS1"
      },
      "source": [
        "## 选择和评估模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ono-wJMrGRS1"
      },
      "source": [
        "你应该对句子嵌入有很好的理解，以及我们可以用它们做些什么。今天，我们使用了两个不同的模型，`all-MiniLM-L6-v2` 和 `quora-distilbert-multilingual`。我们如何知道该使用哪个模型？我们如何知道一个模型是否好用？\n",
        "\n",
        "第一步是知道在哪里发现句子嵌入模型。如果您正在使用开源模型，Hugging Face Hub 允许您[按条件筛选](https://huggingface.co/models?library=sentence-transformers)。社区已经分享了超过 4000 个模型！尽管查看 Hugging Face 上的热门模型是一个很好的指标（例如，我可以看到微软的 Multilingual 5 Large 模型，一个不错的模型），但我们需要更多信息来选择一个模型。\n",
        "\n",
        "[MTEB](https://huggingface.co/spaces/mteb/leaderboard) 让我们不用担心。该排行榜包含多个用于各种任务的评估数据集。让我们快速看一下在选择模型时我们感兴趣的一些标准。\n",
        "\n",
        "* **序列长度** 如前所述，根据预期的用户输入，您可能需要对更长的序列进行编码。例如，如果您要对长文档进行编码，您可能需要使用具有更大序列长度的模型。另一种选择是将文档拆分为多个句子，并分别对每个句子进行编码。\n",
        "* **语言** 排行榜主要包含英语或多语言模型，但您也可以找到其他语言的模型，比如中文、波兰语、丹麦语、瑞典语、德语等。\n",
        "* **嵌入维度** 如前所述，嵌入维度越大，嵌入可以捕获的信息就越多。但是，计算和存储更大的嵌入会更昂贵。\n",
        "* **跨任务的平均指标** 排行榜包含多个任务，例如聚类、重新排序和检索。您可以查看所有任务的平均表现，以了解模型的好坏程度。\n",
        "* **特定任务的指标** 您还可以查看模型在特定任务中的表现。例如，如果您对聚类感兴趣，可以查看模型在聚类任务中的表现。\n",
        "\n",
        "了解模型的目的也是至关重要的。一些模型将是通用模型，而另一些模型，如[Specter 2](https://huggingface.co/allenai/specter2)，则专注于特定任务，比如科学论文。我不会深入讨论排行榜中的所有任务，但你可以查看[MTEB论文](https://arxiv.org/abs/2210.07316)以获取更多信息。让我简要总结一下MTEB。\n",
        "\n",
        "[MTEB论文中的任务图片](https://github.com/osanseviero/hackerllama/blob/main/nbs/blog/posts/sentence_embeddings/mteb.png?raw=1)\n",
        "\n",
        "MTEB 提供了跨八项任务的 56 个数据集的基准测试，并包含 112 种语言。它很容易扩展，以将您的数据集和模型添加到排行榜中。总体而言，它是一个简单易用的工具，可帮助您找到适合您用例的速度与准确度之间的平衡点。\n",
        "\n",
        "今天（2024年1月7日）排名第一的模型是一个大型模型，E5-Mistral-7B-instruct，大小为14.22GB，在56个数据集上的平均分数为66.63。下一个最好的开源模型之一是BGE-Large-en-v1.5，大小只有1.34GB，平均得分为64.23。而BGE的基础模型，甚至更小（0.44GB），质量为63.55！相比之下，text-embedding-ada-002，尽管提供了更大的1536维度嵌入，但质量只有60.99。在MTEB基准测试中排名第23！Cohere提供了质量更好的嵌入，质量为64.47，维度为1024。\n",
        "\n",
        "我建议查看这个2022年的 Twitter 推文，在这个推文中，OpenAI 的嵌入被与其他嵌入进行了比较。结果非常有趣！成本要高出许多个数量级，而质量比较小的模型要低得多。\n",
        "\n",
        "**在这一切中，不要过于专注于一个单一的数字。您应该始终关注您任务的具体指标，以及特定的资源和速度要求**\n",
        "\n",
        "浏览 MTEB 中涵盖的不同任务是很有意思的，可以使你更好地理解句子嵌入的潜在应用\n",
        "\n",
        "* **双语文本挖掘** 这个任务涉及在两组句子中找到最相似的句子，每组句子都是不同语言的。这对机器翻译和跨语言搜索非常重要。\n",
        "* **分类** 在这个应用中，使用句子嵌入训练 logistic 回归分类器进行文本分类任务。\n",
        "* **聚类** 这里，使用句子嵌入训练 k-means 模型，将相似的句子聚合在一起，有助于无监督学习任务。\n",
        "* **句对分类** 这个任务涉及预测一对句子是否相似，比如确定它们是否是重复的或是释义的，有助于释义检测。\n",
        "* **重新排序** 在这种情况下，基于它们与查询句子的相似性重新排列一系列参考文本，改进搜索和推荐系统。\n",
        "* **检索** 这个应用涉及将查询和相关文档嵌入以找到与给定查询最相似的文档，对搜索相关任务至关重要。\n",
        "* **语义相似度** 这个任务侧重于确定一对句子之间的相似性，输出连续的相似度分数，对于释义检测等任务非常有用。\n",
        "* **摘要生成** 这涉及通过计算摘要与参考（人工编写的）摘要之间的相似性来对一组摘要进行评分，这在摘要生成评估中非常重要。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZaeIjikGRS1"
      },
      "source": [
        "## 展示应用程序：在浏览器中运行嵌入模型\n",
        "我们不会进行这个实操，但我想向你展示一种关于嵌入的很酷的应用。Lee Butterman 构建了一个[很酷的应用](https://leebutterman.com/wikipedia-search-by-vibes/)，用户可以使用嵌入在数百万条维基百科文章中进行搜索。**这里特别棒的是，这是离线的：嵌入存储在浏览器中，模型也直接在您的浏览器中运行 - 没有任何数据被发送到服务器！** 🤯\n",
        "\n",
        "数据准备\n",
        "\n",
        "* 我们首先预先计算了一个嵌入数据库。作者使用了一个小型但有效的模型，即 all-minilm-l6-v2。\n",
        "* 6百万个页面 * 384维度 * 每个浮点数占用4字节 = 9.2 GB。这对用户来说下载量相当大。\n",
        "* 作者使用了一种称为[矢量量化](https://en.wikipedia.org/wiki/Vector_quantization)的技术来减小数据库的大小。\n",
        "* 然后将数据导出到一种名为 Arrow 的格式中，这个格式非常紧凑！\n",
        "\n",
        ":::{.温馨提示}\n",
        "\n",
        "不要过于担心这里的具体细节。我们的主要目标是理解这个项目的高层次概念，而不是去实践它；所以如果这是你第一次听到“量化”这个词，也请不要害怕！\n",
        "\n",
        ":::\n",
        "\n",
        "推理阶段\n",
        "\n",
        "* Lee使用了transformers.js，这是一个允许在浏览器中使用JavaScript运行transformers模型的库。这需要具有量化模型。以下是一个示例：\n",
        "\n",
        "```js\n",
        "const extractor = await pipeline('feature-extraction', 'Xenova/all-MiniLM-L6-v2');\n",
        "const output = await extractor('This is a simple test.', { pooling: 'mean', normalize: true });\n",
        "// Tensor {\n",
        "//   type: 'float32',\n",
        "//   data: Float32Array [0.09094982594251633, -0.014774246141314507, ...],\n",
        "//   dims: [1, 384]\n",
        "// }\n",
        "```\n",
        "\n",
        "* `transformers.js` 将所有-MiniLM-L6-v2模型下载到浏览器，并用于在浏览器中计算向量值\n",
        "* 然后使用 [pq.js](https://github.com/lsb/pq.js) 计算距离\n",
        "\n",
        "您可以在 [lee的博客文章](https://www.leebutterman.com/2023/06/01/offline-realtime-embedding-search.html) 中详细了解这个项目。这是嵌入（embeddings）如何在浏览器中使用的一个很好的例子！"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hxsSclNGRS1"
      },
      "source": [
        "## 生态系统\n",
        "\n",
        "嵌入（embeddings）周围的生态系统非常庞大。\"\n",
        "\n",
        "### 基于向量的构建方法：\n",
        "\n",
        "* 有一些很酷的工具，比如`top2vec`和`bertopic`，专门用于构建主题嵌入。\n",
        "* `keybert`是一个允许使用BERT嵌入从文档中提取关键词和关键短语的库。\n",
        "* `setfit`是一个库，允许对Sentence Transformers进行有效的少样本微调，以便将其用于文本分类。\n",
        "\n",
        "### 向量数据库\n",
        "\n",
        "2023年是嵌入式数据库元年。[LangChain 集成部分](https://integrations.langchain.com/vectorstores)显示有65个向量存储库。从Weaviate、Pinecone和Chroma到Redis、ElasticSearch和Postgres等各种工具都有涵盖。嵌入式数据库专注于加速对向量相似性搜索，通常使用近似搜索算法。嵌入式数据库初创公司的新浪潮导致了大量资金的投入。与此同时，一些传统的数据库公司也将向量索引集成到了它们的产品中，例如Cassandra和MongoDB。\n",
        "\n",
        "### 研究\n",
        "\n",
        "嵌入领域的研究也非常活跃。如果你关注MTEB基准测试，它每几周就会发生变化。在这方面的一些参与者包括微软（E5模型）、Cohere、智源研究院（BGE）、阿里巴巴（GTE）、香港大学自然语言处理组（Instructor）以及Jina等众多机构。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTjr_gmaGRS1"
      },
      "source": [
        "## 结论\n",
        "\n",
        "这是一段奇妙的旅程！我们从零开始，深入了解了句子嵌入。我们学到了它们是什么，如何计算它们，如何比较它们，以及如何扩展它们。我们还看到了一些嵌入的酷炫应用，如语义搜索和释义挖掘。希望这篇博客文章让你对句子嵌入有了深刻的理解，知道如何使用它们。这是系列文章的第一部分。还有什么可以学习的呢？\n",
        "\n",
        "* 向量数据库的作用\n",
        "* 如何在更复杂的排名系统中使用嵌入\n",
        "* 主题建模\n",
        "* 多模态\n",
        "* 如何训练自己的向量模型\n",
        "* 关于RAGs的一切\n",
        "\n",
        "有时候需要给每个人留出一些时间！就目前而言，我建议你休息一下，检查一下你的知识。不要犹豫，修改代码并进行调试！如果你喜欢这篇博客文章，请[点亮 GitHub Star](https://github.com/osanseviero/hackerllama) 或分享它！"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HFkHaxnGRS1"
      },
      "source": [
        "## 知识检查\n",
        "\n",
        "1. 为什么 Transformer 模型比 GloVe 或 Word2Vec 在计算向量值方面更为有效？\n",
        "2. BERT 中 [CLS] 标记的作用是什么，它如何帮助计算句子向量值？\n",
        "3. pooler_output 和 ['CLS'] 标记向量之间有什么区别？\n",
        "4. ['CLS'] 池化、最大池化和平均池化之间有什么区别\n",
        "5. Transformer 模型的序列长度限制是多少，我们如何解决这个问题？\n",
        "6. 何时需要对嵌入进行归一化？\n",
        "7. 哪两个向量值的余弦相似度为 -1？为 0 呢？\n",
        "8. 解释 paraphrase_mining 函数的不同参数。\n",
        "9. 在什么情况下你会选择最适合你使用案例的模型？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta0iMq8KGRS1"
      },
      "source": [
        "## 资源\n",
        "\n",
        "这里有许多可用的资源：\n",
        "\n",
        "* [Sentence Transformers](https://www.sbert.net/)\n",
        "* [Hugging Face Hub](https://huggingface.co/models?library=sentence-transformers)\n",
        "* [MTEB Leaderboard](https://huggingface.co/blog/mteb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bifMmQzyGRS2"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "python3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
