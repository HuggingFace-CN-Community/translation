---
date: "01/12/2024"
toc: false
twitter-card: true
title: 大模型社区Llama便利指南
format:
  html:
    comments:
      utterances:
         repo: osanseviero/hackerllama
---

以下是一些在大模型社区里有可能用到的术语。

1. **LocalLlama:** 一个由从业者、研究人员和黑客组成的 [Reddit 社区](https://www.reddit.com/r/LocalLLaMA/)，他们使用机器学习模型进行各种疯狂的测试。

    ![](localllama.jpeg)

2. **LLM:** 大语言模型. 通常是基于Transfomer架构训练的，具有大量的参数...参数量可能达到数十亿甚至是万亿。

3. **Transformer:** 一种在文本任务上表现地非常优异的神经网络架构。它是绝大多数大型语言模型的基础。

4. **GPT:** 一种transformer架构的模型，被训练用于预测句子中的下一个标记。GPT-3是GPT模型的一个例子...谁知道呢

    4.1 **Auto-regressive:** 一种机器学习模型，逐个标记地生成文本。它是自回归的，因为它使用自己的预测来生成下一个标记。例如，模型可能以"今天的天气"为输入，生成下一个标记"是"。然后，它将"今天的天气是"作为输入，并生成下一个标记"晴朗"。它将继续使用"今天的天气是晴朗"作为输入，并生成下一个标记"和"。以此类推。


5. **Token:** 模型不理解单词，它理解数字。当我们收到一系列单词时，我们将它们转换为数字。有时我们将单词拆分为片段，例如将"tokenization"拆分为"token"和"ization"。这是因为模型具有有限的词汇量。标记是模型能够理解的语言的最小单位。

6. **Context length:**  模型可以同时使用的标记数量。上下文长度越大，模型需要的内存越多，运行速度越慢。例如，Llama 2可以处理最多4096个标记。

    6.1 **LLaMA:** Meta训练的一个预训练模型，最开始是共享给一些私人访问的团体，然后泄露出来。它引发了许多令人兴奋的项目。 🦙

    6.2 **Llama 2:** Meta发布的一个开放访问的预训练模型。它引发了另一波非常酷的项目，而且这一次没有泄露！许可证在技术上不是开源的，但对商业用途仍然相当开放和宽容。🦙🦙

    6.3 **RoPE:**  一种允许显著扩展模型上下文长度的技术

    6.4 **SuperHot:** 一种允许通过进行一些最小的额外训练来进一步扩展基于RoPE的模型上下文长度的技术。

7. **Pre-training:** 在非常大的数据集（数万亿标记）上训练模型，以学习语言的结构。通常情况下，通过从互联网上抓取大型数据集并在其上训练模型来实现。这称为预训练。这不需要标记的数据！这是在微调之前完成的。预训练模型的示例包括GPT-3、Llama 2和Mistral。

    7.1 **Mistral 7B:** Mistral训练的一个预训练模型。通过种子分享
    ![](mistral.png)

    7.2 **Phi 2:** 由Microsoft训练的预训练模型。它只有2.7B参数，但对于其大小来说相当不错！它是用很少的数据（教科书）训练的，显示了高质量数据的力量。

    7.3 **transformers:** 一个Python库，用于访问社区共享的模型。它允许您下载预训练模型并根据自己的需要进行微调。

    7.4 **Base vs conversational:** 预训练模型并非专门训练为问答形式。如果尝试直接使用基础模型（例如GPT-3、Mistral、Llama）进行对话，其表现可能不如经过微调的对话变体（ChatGPT、Mistral Instruct、Llama Chat）好。在进行基准测试时，要将基础模型与基础模型进行比较，将对话模型与对话模型进行比较。

8. **Fine-tuning:** 数据集上训练模型，以学习特定任务。这是在预训练之后完成的。想象一下，你手头有一些钱，作为一个GPU较差的朋友。与其从头开始训练一个模型，不如选择一个预训练（基础）模型并对其进行微调。通常选择几百到几千个样本的小数据集。然后将其传递给模型并在其上进行训练。这称为微调。其目的是得到一个在特定任务上具有强大理解能力的模型。例如，你可以用你的推文对模型进行微调，使其生成类似于你的推文！（但请不要这样做）。微调可以在普通的游戏笔记本电脑上完成！微调模型的示例包括ChatGPT、Vicuna和Mistral Instruct。

    8.1 **Mistral 7B Instruct:** Mistral 7B的微调版本。

    8.2 **Vicuna:** 一种可爱的动物，也是一个微调模型。它从LLaMA-13B开始，通过使用ChatGPT的用户对话数据进行微调。

    8.3 **Number of parameters:** 注意8.2中的"-13B"。这是模型中的参数数量。每个参数都是模型的一部分，是一个数字（具有一定的精度），在预训练和微调中进行学习以最小化误差。

      ![](gpu_poor.png)

9. **Prompt:** 为了开始生成文本，你给模型提供的几个词。例如，如果你想生成一首诗，你可以将诗的第一行作为提示提供给模型。模型将然后生成诗的其余部分！
10. **Zero-shot:** 一种在没有微调的情况下生成文本的提示类型。模型没有针对任何特定任务进行训练。它只在大量文本数据集上进行了训练。例如，你可以给模型提供一首诗的第一行，并要求它生成诗的其余部分。尽管模型以前从未见过诗歌，但它会尽力生成一首诗！在使用ChatGPT时，通常进行零-shot生成！

    ```
    User: Write a poem about a llama
    _______________
    Model:
    Graceful llama, in Andean air,
    Elegant stride, woolly flair.
    Mountains echo, mystic charm,
    Llama's gaze, a tranquil balm.
    ```


11. **Few-shot:**  一种用于在微调中生成文本的提示类型。我们向模型提供了一些示例。这可以显著提高生成的文本质量！

    ```
    User
    Input:

    Text: "The cat sat on the mat."
    Label: Sentence about an animal.

    Text: "The sun is incredibly bright today."
    Label: Sentence about weather.

    Classification Task:
    Classify the following text - "Rainy days make me want to stay in bed."

    Output:
    Label: Sentence about weather.

    Text: "Rainy days make me want to stay in bed."
    __________________
    Model
    Label: Sentence about weather.
    ```

12. **Instruct-tuning:** 一种使用指令生成文本的微调类型，以实现更受控制的行为，生成响应或执行任务。

    12.1 **Alpaca:** 由OpenAI API生成的52,000条指令的数据集。它掀起了人们使用OpenAI生成合成数据进行指导微调的浪潮。生成这个数据集的成本约为500美元。

    12.2 **LIMA:** 一种演示以很少的示例表现出强大性能的模型。它展示了添加更多数据并不总是与更好的质量相关联。

13. **RLHF (Reinforcement Learning with Human Feedback):** 一种使用强化学习（RL）和人类生成的反馈进行微调的方法。通过引入人类反馈，最终模型在对话等方面表现出色！它从一个基础模型开始生成大量对话。然后人们对答案进行评分（偏好）。这些偏好用于训练一个奖励模型，为给定文本生成一个分数。使用强化学习，初始LM被训练以最大化奖励模型生成的分数。在这里[详细了解]。(https://huggingface.co/blog/rlhf).

    13.1 **RL:** 强化学习是一种使用奖励来训练模型的机器学习类型。例如，可以通过在赢得游戏时给予奖励，输掉游戏时给予惩罚来训练模型玩游戏。模型将学会赢得游戏！

    13.2. **Reward Model:** 用于生成奖励的模型。例如，可以训练一个模型为游戏生成奖励。模型将学会生成对游戏有利的奖励！

    13.3 **ChatGPT:** 经过RLHF微调的GPT-3模型，非常擅长对话。

    13.4 **AIF**: 与人类反馈相反的一种选择...AI反馈！

14. **PPO:** 一种用于训练模型的强化学习算法,在RLHF中使用。

15. **DPO:** 一种最大化奖励模型需求的算法。它大大简化了RLHF管道。
    15.1 **Zephyr:** 一个使用DPO训练的7B Mistral-based模型。它具有类似于Llama 2 Chat模型的70B参数的能力。它还提供了一个很好的[食谱手册](https://github.com/huggingface/alignment-handbook/tree/main)。

    15.2 **Notus:** Zephyr的训练变体，但具有更好的过滤和修复数据。效果更好！

    15.3 **Overfitting:** 在机器学习中，当模型对训练数据学得太好，捕捉噪音和不适用于新的、未见过的数据，导致在实际任务中性能不佳时常发生

    15.4 **DPO Overfits** 尽管DPO在一次行为后表现出过拟合行为，但它并不影响对话评估的下游性能。难道我们的机器学习老师在说过拟合不好的时候骗了我们吗？

    15.5 **IPO:** 一种更简单且不容易过拟合的DPO目标变化。

    15.6. **KTO:** 相比PPO、DPO和IPO需要接受与拒绝的生成对，KTO只需要二进制标签（接受或拒绝），因此可以处理更多数据。

    15.7 **trl:**  一个允许使用DPO、IPO、KTO等训练模型的库

16. **Open LLM Leaderboard:** 一个排行榜，你可以在上面找到许多开放访问的LLM [Large Language Models leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)的基准结果。 

    17.1 **Benchmark:** 运行以比较不同模型性能的测试。例如，你可以运行基准测试来比较不同模型在特定任务上的性能。

    17.2 **TruthfulQA:** 一个不太理想的基准，用于衡量模型生成真实答案的能力。

    17.3 **Conversational models:**  LLM排行榜主要用于比较基础模型，而不是对话模型。它仍然提供一些关于对话模型的有用信息，但这不应该是最终评估它们的方式。

    ![](benchmark.png)

17. **Chatbot Arena:**  一个[众包开放基准](https://lmsys.org/blog/2023-05-03-arena/) ，用于收集人类偏好。用于比较对话模型。

    ![](chatbot_arena.png)

18. **MT-Bench:**  一个包含八个领域的160个问题的多轮基准。每个响应都由GPT-4进行评估。（存在限制...如果模型比GPT-4更好会发生什么？）

19. **Mixture-of-Experts (MoE):** 一种模型架构，其中一些（密集）层被一组专家替换。每个专家是一个小型神经网络。有一个小型网络，路由器，它决定为每个令牌使用哪个专家在这里(可以找到[更多信息](https://huggingface.co/blog/moe))

    * MoE不是一个集成。
    * 如果说一个MoE有8个专家，这意味着每个被替换的密集层都被8个专家替换。如果有3个被替换的层，那么总共有24个专家！
    * 对于定的句子，“hello world”，“hello”可能被发送到专家1和2，而“world”则发送到专家2和4。
    * MoE中的专家不专门针对某个任务。它们都在相同的任务上进行训练，只是得到不同的令牌！有时它们确实在某些类型的令牌上专业化，如ST-MoE论文中所示的表格。

    ![](moe.png)

    19.1 **GPT-4:** 一种相当不错的模型，但我们不知道它是什么。有传言说它是一个MoE。

    19.2 **Mixtral:** 由Mistral发布的MoE模型，拥有470亿参数，但每次只使用120亿参数，使其非常高效。

20. **Model Merging:** 一种允许将相同架构的多个模型合并为单个模型的技术，（在[这里](https://huggingface.co/blog/mlabonne/merge-models)可以找到更多信息）。
   
    20.1 **Mergekit:** 一个快速合并仓库的开源工具。
    
    20.2 **Averaging:** 最基本的合并技术。选择两个模型，平均它们的权重。不知何故，它似乎有点奏效！

    20.3 **Frankenmerge:** 允许连接来自不同LLM的层，使你可以做一些疯狂的事情。

    20.4 **Goliath-120B:** 一个Frankenmerge，将两个Llama 70B模型合并为一个120B模型。

    20.5 **MoE Merging:** （对此不太确定）mergekit中的实验性分支，允许构建类似MoE的模型，结合不同的模型。您可以指定每个专家处理哪些类型的提示，从而实现专家任务专业化。

    20.6 **Phixtral:**  Phi 2 DPO和Dolphin 2 Phi 2的MoE合并。

    ![](merge.jpeg)

21. **Local LLMs:**  如果模型足够小，我们可以在计算机或甚至手机上运行它们！

    21.1 **TinyLlama:** 一个在3万亿令牌上预训练1.1B Llama模型的项目。

    21.2 **Cognitive Computations:**  由Eric Hartford领导的一个社区，正在微调一堆模型。
    
    21.3 **Uncensored models:** 许多模型在决策制定过程中具有某种强烈的定向，阻止了一些操作，如要求Llama终止Linux进程。训练未经审查的模型的目标是消除在模型微调决策制定过程中固有的特定偏见。

      ![](process.png)

    21.4 **llama.cpp:** 一个在C++中使用类似Llama模型的工具。

    21.5 **GGUF:** 由llama.cpp引入的一种存储模型的格式。它替代了旧的文件格式GGML。

    21.6 **ggml:** ML中的张量库，允许项目使用llama.cpp和whisper.cpp等。

    21.7 **Georgi Gerganov:** llama.cpp和ggml的创作者！

    21.8 **Whisper:** 最先进的开源语音到文本模型。

    21.9 **OpenAI:**  一家进行闭源AI的公司（开个玩笑，他们开源了Whisper！）。

    21.10 **MLX:** 一个适用于Apple设备的新框架，允许轻松进行推断和模型微调。

22. (A) **Local LLM tools:**  如果您不知道如何编码，有一些工具可能会很有用。

    22.1 **Oobabooga:** 一个简单的Web应用程序，允许您在无需编码的情况下使用模型。非常易于使用！

    22.2 **LM Studio:**  一个在本地完全离线运行模型的高级应用程序。

    22.3 **ollama:** 一个在其基础上提供多个Web/Desktop应用程序和终端集成的开源工具。

    22.4 **ChatUI:** 一个使用开源模型的开源UI。

23. **Quantization:** 一种通过减小模型权重的精度来减小模型大小的技术。例如，我们可以将模型的精度从32位减小到8位，从而将模型的大小减小4倍！模型可能会（有时）准确性降低，但它将更小，可以在手机等较小的设备上运行。

    23.1 **TheBloke:**  一个量化模型的家伙。一旦有新模型发布，他就会对其进行量化！查看他的[HF个人资料](https://huggingface.co/TheBloke).

    ![](thebloke.png)

    23.2 **Hugging Face:**  一个分享和查找开放访问模型、数据集和演示的平台。它还是一家构建不同OS库的公司（也是我工作的地方！）

    23.3. **Facehugger:**  Alien电影中的一种怪物。它应该也是一个开源工具。不过目前还没有。

    23.4. **GPTQ:**  一种流行的量化技术。

    23.5 **AWQ:** 另一种流行的量化技术。

    23.6 **EXL2:** 由名为exllamav2的库使用的一种不同的量化格式。

    23.7 **LASER:** 一种通过减小特定矩阵的秩来减小模型大小并提高性能的技术。不需要额外的训练。

24. **PEFT:**  一系列方法，允许在不修改所有参数的情况下进行模型微调。通常，您冻结模型，添加一小组参数，然后仅修改它。这减少了计算量，并且您可以获得非常好的结果！

    24.1 **peft:**  用于执行PEFT的流行开源库！它被用于其他项目，如trl。

    24.2 **adapters:** 用于执行PEFT的另一个流行库。

    24.3.**unsloth**: 使用QLoRA进行PEFT的更高级库。

    24.4. **LoRA:** 最受欢迎的PEFT技术之一。它添加了低秩“更新矩阵”。基本模型被冻结，只训练更新矩阵。这可用于图像分类、将Stable Diffusion的概念教给您的宠物或LLM微调。
    
25. **QLoRA:** 一种将LoRA与量化相结合的技术，因此我们使用4位量化并仅更新LoRA参数！这使得可以在GPU资源较为有限的情况下对模型进行微调。

    25.1. **Tim Dettmers:** 一位研究人员，致力于PEFT的研究并创建了QLoRA微调方法。

    25.2. **Guanaco (model):** 通过QLoRA微调方法得到的LLaMA微调模型

    ![](qlora.png)

26. **axolotl:** 一种可爱的动物，也是一种简化微调过程的高级工具，包括对QLoRA等微调方法的支持。

27. **Nous Research**: 一家由运营自己的Discord社区频道而发展起来的公司，致力于发布一系列酷炫的模型。

28. **Multimodal:**  一个能够处理多种模态的单一模型。例如，一个能够同时生成文本和图像的模型。或者一个能够同时生成文本和音频的模型。又或者一个能够同时生成文本、图像和音频的模型。又或者一个能够同时生成文本、图像、音频、视频、气味、味道、感觉、思维、梦境、记忆、意识、灵魂、宇宙、神、多宇宙和全宇宙的模型。（感谢ChatGPT的幻想）
    28.1 **Hallucination:** 模型生成的回答是连贯的，但实际上并不准确，模型会将现实不存在的信息自我逻辑自洽，导致虚假信息或想象中的情境的产生，就像上面的例子一样！

    28.2 **LlaVA:** 一个多模态模型，能够接收图像和文本作为输入，并生成文本响应。

29. **Bagel:** 一个过程，它混合了大量的监督微调和偏好数据。它使用不同的提示格式，使模型对各种提示更加灵活适用。

30. **Code Models:** 专门为代码任务而预训练的语言模型。

    30.1. **Big Code Models Leaderboard:** 一个用于比较在HumanEval数据集中进行测评的代码模型的效果的[排行榜](https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard)。

    30.2. **HumanEval:** 一个非常小的数据集，包含164个Python编程问题。这些问题已经在MultiPL-E中被翻译成18种编程语言。

    30.3 **BigCode:** 一个致力于代码相关模型和数据集的开放的科学合作团体。

    ![](bigcode.jpeg)

    30.4 **The Stack:** 一个包含6.4TB数据量的合法许可的代码数据集，涵盖了358种编程语言。

    30.5 **Code Llama:** 最佳的基础代码模型。它基于Llama2微调。

    ![](codellama.jpeg)

    30.6 **WizardLM:** 来自微软的一个研究团队……同时也是一个Discord社群。

    30.7 **WizardCoder:** WizardLM发布的代码模型。其架构基于 Llama。

31. **Flash Attention:** 一种近似注意力的算法，能够实现巨大的LLM推理加速效果

    31.1 **Flash Attention 2:** 对该注意力算法的升级版本，可提供更好的LLM推理加速效果。

    31.2. **Tri Dao:** flash attention技术的作者，是注意力生态中的传奇人物。

希望你能喜欢！请随时在下面的评论中提出新的术语或者更正意见。随着新的术语出现，我将不断更新这篇文章。
